{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7461920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79154a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75002b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9307b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7055cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-10-15 13:14:34,317:jax._src.xla_bridge:794: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document represents a template for a simple breeding simulation for selection. It is meant to be a guide to developing my complex or flexible breeding designs.\n",
      "--- Running 10-Cycle Phenotypic Selection Experiment ---\n",
      "\n",
      "Step 1: Initializing founder population...\n",
      "Step 2: Initializing trait architecture...\n",
      "  - Founder population size: 100\n",
      "  - Selection: Top 20 individuals\n",
      "  - Offspring per cycle: 200\n",
      "\n",
      "Step 3: Running burn-in cycle to stabilize population size...\n",
      "\n",
      "Step 4: Running remaining 19 selection cycles (JIT compiling...)\n",
      "\n",
      "--- Results ---\n",
      "Generation 00 Mean TBV: 0.816\n",
      "Generation 01 Mean TBV: 0.816 (Gain: +0.000)\n",
      "Generation 02 Mean TBV: 2.206 (Gain: +1.390)\n",
      "Generation 03 Mean TBV: 3.775 (Gain: +1.569)\n",
      "Generation 04 Mean TBV: 5.289 (Gain: +1.514)\n",
      "Generation 05 Mean TBV: 6.590 (Gain: +1.301)\n",
      "Generation 06 Mean TBV: 7.983 (Gain: +1.394)\n",
      "Generation 07 Mean TBV: 9.112 (Gain: +1.129)\n",
      "Generation 08 Mean TBV: 10.077 (Gain: +0.965)\n",
      "Generation 09 Mean TBV: 10.777 (Gain: +0.700)\n",
      "Generation 10 Mean TBV: 11.417 (Gain: +0.640)\n",
      "Generation 11 Mean TBV: 11.958 (Gain: +0.541)\n",
      "Generation 12 Mean TBV: 12.409 (Gain: +0.451)\n",
      "Generation 13 Mean TBV: 12.898 (Gain: +0.490)\n",
      "Generation 14 Mean TBV: 13.360 (Gain: +0.461)\n",
      "Generation 15 Mean TBV: 13.782 (Gain: +0.422)\n",
      "Generation 16 Mean TBV: 14.220 (Gain: +0.438)\n",
      "Generation 17 Mean TBV: 14.535 (Gain: +0.315)\n",
      "Generation 18 Mean TBV: 14.894 (Gain: +0.359)\n",
      "Generation 19 Mean TBV: 15.138 (Gain: +0.244)\n",
      "Generation 20 Mean TBV: 15.476 (Gain: +0.338)\n",
      "\n",
      "--- Total Genetic Gain over 20 generations: +14.660 ---\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from chewc.structs import *\n",
    "from chewc.pheno import *\n",
    "from chewc.select import *\n",
    "from chewc.cross import *\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from functools import partial\n",
    "from typing import Tuple\n",
    "\n",
    "print(\"This document represents a template for a simple breeding simulation for selection. It is meant to be a guide to developing my complex or flexible breeding designs.\")\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"n_select\", \"n_offspring\", \"max_crossovers\"))\n",
    "def selection_step(\n",
    "    carry: BreedingState,\n",
    "    _, # Placeholder for lax.scan's iteration number\n",
    "    trait: Trait,\n",
    "    genetic_map: GeneticMap,\n",
    "    heritabilities: jnp.ndarray,\n",
    "    n_select: int,\n",
    "    n_offspring: int,\n",
    "    max_crossovers: int\n",
    ") -> Tuple[BreedingState, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Executes one full cycle of selection and breeding.\n",
    "    This function is designed to be the body of a lax.scan loop.\n",
    "    \"\"\"\n",
    "    key, pheno_key, mating_key, cross_key = jax.random.split(carry.key, 4)\n",
    "    current_pop = carry.population\n",
    "\n",
    "    # 1. Evaluate the population\n",
    "    phenotypes, tbvs = calculate_phenotypes(\n",
    "        key=pheno_key, population=current_pop, trait=trait, heritability=heritabilities\n",
    "    )\n",
    "    mean_tbv = jnp.mean(tbvs[:, 0]) # Track genetic gain for trait 1\n",
    "\n",
    "    # 2. Select top parents\n",
    "    selected_parents = select_top_k(current_pop, phenotypes[:, 0], k=n_select)\n",
    "\n",
    "    # 3. Generate a random mating plan\n",
    "    pairings = random_mating(mating_key, n_parents=n_select, n_crosses=n_offspring)\n",
    "    mother_indices, father_indices = pairings[:, 0], pairings[:, 1]\n",
    "\n",
    "    # 4. Create the next generation\n",
    "    mothers_geno = selected_parents.geno[mother_indices]\n",
    "    fathers_geno = selected_parents.geno[father_indices]\n",
    "    mothers_ibd = selected_parents.ibd[mother_indices]\n",
    "    fathers_ibd = selected_parents.ibd[father_indices]\n",
    "\n",
    "    # Vectorize the crossing operation over all pairs\n",
    "    vmapped_cross = jax.vmap(\n",
    "        cross_pair, in_axes=(0, 0, 0, 0, 0, None, None)\n",
    "    )\n",
    "    offspring_keys = jax.random.split(cross_key, n_offspring)\n",
    "    offspring_geno, offspring_ibd = vmapped_cross(\n",
    "        offspring_keys, mothers_geno, fathers_geno, mothers_ibd, fathers_ibd, genetic_map, max_crossovers\n",
    "    )\n",
    "\n",
    "    # 5. Form the new population and update the state\n",
    "    new_generation = carry.generation + 1\n",
    "    new_ids = jnp.arange(n_offspring, dtype=jnp.int32) + carry.next_id\n",
    "    new_meta = jnp.stack(\n",
    "        [\n",
    "            new_ids,\n",
    "            selected_parents.meta[mother_indices, 0],\n",
    "            selected_parents.meta[father_indices, 0],\n",
    "            jnp.full((n_offspring,), new_generation, dtype=jnp.int32),\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    new_population = Population(geno=offspring_geno, ibd=offspring_ibd, meta=new_meta)\n",
    "\n",
    "    next_state = BreedingState(\n",
    "        population=new_population,\n",
    "        key=key,\n",
    "        generation=new_generation,\n",
    "        next_id=carry.next_id + n_offspring\n",
    "    )\n",
    "\n",
    "    # The scan function requires a `(carry, output)` return signature.\n",
    "    # `carry` is the state for the next iteration.\n",
    "    # `output` is the data we want to collect at each step.\n",
    "    return next_state, mean_tbv\n",
    "\n",
    "\n",
    "def run_simulation_cycles(\n",
    "    initial_state: BreedingState,\n",
    "    trait: Trait,\n",
    "    genetic_map: GeneticMap,\n",
    "    heritabilities: jnp.ndarray,\n",
    "    n_cycles: int,\n",
    "    n_select: int,\n",
    "    n_offspring: int,\n",
    "    max_crossovers: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the entire multi-cycle simulation using lax.scan for maximum performance.\n",
    "    \"\"\"\n",
    "    # `lax.scan` requires the body function to be a callable that takes\n",
    "    # (carry, x) and returns (new_carry, y). We use a lambda to fix the\n",
    "    # static arguments and data that doesn't change over the loop.\n",
    "    scan_fn = lambda carry, _: selection_step(\n",
    "        carry,\n",
    "        _,\n",
    "        trait=trait,\n",
    "        genetic_map=genetic_map,\n",
    "        heritabilities=heritabilities,\n",
    "        n_select=n_select,\n",
    "        n_offspring=n_offspring,\n",
    "        max_crossovers=max_crossovers\n",
    "    )\n",
    "\n",
    "    # Run the scan. The `None` is a placeholder for the `xs` array,\n",
    "    # as we only care about the number of iterations (`length`).\n",
    "    final_state, tbv_history = lax.scan(\n",
    "        scan_fn, initial_state, None, length=n_cycles\n",
    "    )\n",
    "\n",
    "    return final_state, tbv_history\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Main execution script\n",
    "# ----------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Running 10-Cycle Phenotypic Selection Experiment ---\")\n",
    "\n",
    "    # --- Simulation Parameters ---\n",
    "    N_FOUNDERS = 100\n",
    "    N_SELECT = 20\n",
    "    N_OFFSPRING = 200 # Population size is kept constant after the first cycle\n",
    "    N_CYCLES = 20\n",
    "\n",
    "    N_CHR, N_LOCI = 5, 1000\n",
    "    MAX_CROSSOVERS = 10\n",
    "    SEED = 42\n",
    "\n",
    "    key = jax.random.PRNGKey(SEED)\n",
    "\n",
    "    # --- Setup ---\n",
    "    key, pop_key, trait_key = jax.random.split(key, 3)\n",
    "    print(\"\\nStep 1: Initializing founder population...\")\n",
    "    founder_pop, genetic_map = quick_haplo(\n",
    "        key=pop_key, n_ind=N_FOUNDERS, n_chr=N_CHR, seg_sites=N_LOCI,\n",
    "        inbred=False, chr_length=1.0\n",
    "    )\n",
    "\n",
    "    print(\"Step 2: Initializing trait architecture...\")\n",
    "    trait_architecture = add_trait(\n",
    "        key=trait_key, founder_pop=founder_pop, n_qtl_per_chr=50,\n",
    "        mean=jnp.array([0.0]), var_a=jnp.array([1.0]), var_d=jnp.array([0.0]),\n",
    "        sigma=jnp.array([[10.0]])\n",
    "    )\n",
    "    heritabilities = jnp.array([0.9])\n",
    "\n",
    "    # --- Initial State ---\n",
    "    initial_state = BreedingState(\n",
    "        population=founder_pop,\n",
    "        key=key,\n",
    "        generation=0,\n",
    "        next_id=N_FOUNDERS # Next available individual ID\n",
    "    )\n",
    "    print(f\"  - Founder population size: {N_FOUNDERS}\")\n",
    "    print(f\"  - Selection: Top {N_SELECT} individuals\")\n",
    "    print(f\"  - Offspring per cycle: {N_OFFSPRING}\")\n",
    "\n",
    "    # --- Burn-in Cycle ---\n",
    "    print(\"\\nStep 3: Running burn-in cycle to stabilize population size...\")\n",
    "    \n",
    "    # Calculate initial TBV for generation 0\n",
    "    key, initial_pheno_key = jax.random.split(initial_state.key)\n",
    "    _, initial_tbvs = calculate_phenotypes(\n",
    "        key=initial_pheno_key, population=initial_state.population,\n",
    "        trait=trait_architecture, heritability=heritabilities\n",
    "    )\n",
    "    initial_mean_tbv = jnp.mean(initial_tbvs[:, 0])\n",
    "\n",
    "    # Run the first selection step manually\n",
    "    burn_in_state, first_cycle_tbv = selection_step(\n",
    "        carry=initial_state,\n",
    "        _ = 0, # Placeholder for iteration number\n",
    "        trait=trait_architecture,\n",
    "        genetic_map=genetic_map,\n",
    "        heritabilities=heritabilities,\n",
    "        n_select=N_SELECT,\n",
    "        n_offspring=N_OFFSPRING,\n",
    "        max_crossovers=MAX_CROSSOVERS,\n",
    "    )\n",
    "\n",
    "    # --- Run Simulation ---\n",
    "    print(f\"\\nStep 4: Running remaining {N_CYCLES - 1} selection cycles (JIT compiling...)\\n\")\n",
    "    \n",
    "    # Run the rest of the simulation using lax.scan\n",
    "    final_state, tbv_history = run_simulation_cycles(\n",
    "        initial_state=burn_in_state,\n",
    "        trait=trait_architecture,\n",
    "        genetic_map=genetic_map,\n",
    "        heritabilities=heritabilities,\n",
    "        n_cycles=N_CYCLES - 1,\n",
    "        n_select=N_SELECT,\n",
    "        n_offspring=N_OFFSPRING,\n",
    "        max_crossovers=MAX_CROSSOVERS,\n",
    "    )\n",
    "\n",
    "    # --- Report Results ---\n",
    "    print(\"--- Results ---\")\n",
    "    print(f\"Generation 00 Mean TBV: {initial_mean_tbv:.3f}\")\n",
    "    print(f\"Generation 01 Mean TBV: {first_cycle_tbv:.3f} (Gain: {first_cycle_tbv - initial_mean_tbv:+.3f})\")\n",
    "\n",
    "    # Combine initial TBV, burn-in TBV, and the history from the scan\n",
    "    full_history = jnp.concatenate([jnp.array([initial_mean_tbv, first_cycle_tbv]), tbv_history])\n",
    "    for i, tbv in enumerate(tbv_history, 2):\n",
    "        gain = tbv - full_history[i-1]\n",
    "        print(f\"Generation {i:02d} Mean TBV: {tbv:.3f} (Gain: {gain:+.3f})\")\n",
    "\n",
    "    total_gain = tbv_history[-1] - initial_mean_tbv\n",
    "    print(f\"\\n--- Total Genetic Gain over {N_CYCLES} generations: {total_gain:+.3f} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b948a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fcad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b433a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c575383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a1b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b548e319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glect/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import warp: No module named 'warp'\n",
      "Failed to import mujoco_warp: No module named 'warp'\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import chex\n",
    "import numpy as np\n",
    "from flax import struct\n",
    "from functools import partial\n",
    "from typing import Optional, Tuple, Union, Any\n",
    "from gymnax.environments import environment, spaces\n",
    "from brax import envs\n",
    "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
    "import navix as nx\n",
    "\n",
    "\n",
    "class GymnaxWrapper(object):\n",
    "    \"\"\"Base class for Gymnax wrappers.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    # provide proxy access to regular attributes of wrapped object\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "\n",
    "class FlattenObservationWrapper(GymnaxWrapper):\n",
    "    \"\"\"Flatten the observations of the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env: environment.Environment):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def observation_space(self, params) -> spaces.Box:\n",
    "        assert isinstance(\n",
    "            self._env.observation_space(params), spaces.Box\n",
    "        ), \"Only Box spaces are supported for now.\"\n",
    "        return spaces.Box(\n",
    "            low=self._env.observation_space(params).low,\n",
    "            high=self._env.observation_space(params).high,\n",
    "            shape=(np.prod(self._env.observation_space(params).shape),),\n",
    "            dtype=self._env.observation_space(params).dtype,\n",
    "        )\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def reset(\n",
    "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
    "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        obs = jnp.reshape(obs, (-1,))\n",
    "        return obs, state\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def step(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: environment.EnvState,\n",
    "        action: Union[int, float],\n",
    "        params: Optional[environment.EnvParams] = None,\n",
    "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
    "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
    "        obs = jnp.reshape(obs, (-1,))\n",
    "        return obs, state, reward, done, info\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class LogEnvState:\n",
    "    env_state: environment.EnvState\n",
    "    episode_returns: float\n",
    "    episode_lengths: int\n",
    "    returned_episode_returns: float\n",
    "    returned_episode_lengths: int\n",
    "    timestep: int\n",
    "\n",
    "\n",
    "class LogWrapper(GymnaxWrapper):\n",
    "    \"\"\"Log the episode returns and lengths.\"\"\"\n",
    "\n",
    "    def __init__(self, env: environment.Environment):\n",
    "        super().__init__(env)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def reset(\n",
    "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
    "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
    "        obs, env_state = self._env.reset(key, params)\n",
    "        state = LogEnvState(env_state, 0, 0, 0, 0, 0)\n",
    "        return obs, state\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def step(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: environment.EnvState,\n",
    "        action: Union[int, float],\n",
    "        params: Optional[environment.EnvParams] = None,\n",
    "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "        new_episode_return = state.episode_returns + reward\n",
    "        new_episode_length = state.episode_lengths + 1\n",
    "        state = LogEnvState(\n",
    "            env_state=env_state,\n",
    "            episode_returns=new_episode_return * (1 - done),\n",
    "            episode_lengths=new_episode_length * (1 - done),\n",
    "            returned_episode_returns=state.returned_episode_returns * (1 - done)\n",
    "            + new_episode_return * done,\n",
    "            returned_episode_lengths=state.returned_episode_lengths * (1 - done)\n",
    "            + new_episode_length * done,\n",
    "            timestep=state.timestep + 1,\n",
    "        )\n",
    "        info[\"returned_episode_returns\"] = state.returned_episode_returns\n",
    "        info[\"returned_episode_lengths\"] = state.returned_episode_lengths\n",
    "        info[\"timestep\"] = state.timestep\n",
    "        info[\"returned_episode\"] = done\n",
    "        return obs, state, reward, done, info\n",
    "\n",
    "\n",
    "class BraxGymnaxWrapper:\n",
    "    def __init__(self, env_name, backend=\"positional\"):\n",
    "        env = envs.get_environment(env_name=env_name, backend=backend)\n",
    "        env = EpisodeWrapper(env, episode_length=1000, action_repeat=1)\n",
    "        env = AutoResetWrapper(env)\n",
    "        self._env = env\n",
    "        self.action_size = env.action_size\n",
    "        self.observation_size = (env.observation_size,)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        state = self._env.reset(key)\n",
    "        return state.obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        next_state = self._env.step(state, action)\n",
    "        return next_state.obs, next_state, next_state.reward, next_state.done > 0.5, {}\n",
    "\n",
    "    def observation_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=-jnp.inf,\n",
    "            high=jnp.inf,\n",
    "            shape=(self._env.observation_size,),\n",
    "        )\n",
    "\n",
    "    def action_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(self._env.action_size,),\n",
    "        )\n",
    "\n",
    "class NavixGymnaxWrapper:\n",
    "    def __init__(self, env_name):\n",
    "        self._env = nx.make(env_name)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        timestep = self._env.reset(key)\n",
    "        return timestep.observation, timestep\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        timestep = self._env.step(state, action)\n",
    "        return timestep.observation, timestep, timestep.reward, timestep.is_done(), {}\n",
    "\n",
    "    def observation_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=self._env.observation_space.minimum,\n",
    "            high=self._env.observation_space.maximum,\n",
    "            shape=(np.prod(self._env.observation_space.shape),),\n",
    "            dtype=self._env.observation_space.dtype,\n",
    "        )\n",
    "\n",
    "    def action_space(self, params):\n",
    "        return spaces.Discrete(\n",
    "            num_categories=self._env.action_space.maximum.item() + 1,\n",
    "        )\n",
    "\n",
    "\n",
    "class ClipAction(GymnaxWrapper):\n",
    "    def __init__(self, env, low=-1.0, high=1.0):\n",
    "        super().__init__(env)\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        \"\"\"TODO: In theory the below line should be the way to do this.\"\"\"\n",
    "        # action = jnp.clip(action, self.env.action_space.low, self.env.action_space.high)\n",
    "        action = jnp.clip(action, self.low, self.high)\n",
    "        return self._env.step(key, state, action, params)\n",
    "\n",
    "\n",
    "class TransformObservation(GymnaxWrapper):\n",
    "    def __init__(self, env, transform_obs):\n",
    "        super().__init__(env)\n",
    "        self.transform_obs = transform_obs\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        return self.transform_obs(obs), state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
    "        return self.transform_obs(obs), state, reward, done, info\n",
    "\n",
    "\n",
    "class TransformReward(GymnaxWrapper):\n",
    "    def __init__(self, env, transform_reward):\n",
    "        super().__init__(env)\n",
    "        self.transform_reward = transform_reward\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
    "        return obs, state, self.transform_reward(reward), done, info\n",
    "\n",
    "\n",
    "class VecEnv(GymnaxWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.reset = jax.vmap(self._env.reset, in_axes=(0, None))\n",
    "        self.step = jax.vmap(self._env.step, in_axes=(0, 0, 0, None))\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizeVecObsEnvState:\n",
    "    mean: jnp.ndarray\n",
    "    var: jnp.ndarray\n",
    "    count: float\n",
    "    env_state: environment.EnvState\n",
    "\n",
    "\n",
    "class NormalizeVecObservation(GymnaxWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=jnp.zeros_like(obs),\n",
    "            var=jnp.ones_like(obs),\n",
    "            count=1e-4,\n",
    "            env_state=state,\n",
    "        )\n",
    "        batch_mean = jnp.mean(obs, axis=0)\n",
    "        batch_var = jnp.var(obs, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            env_state=state.env_state,\n",
    "        )\n",
    "\n",
    "        return (obs - state.mean) / jnp.sqrt(state.var + 1e-8), state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "\n",
    "        batch_mean = jnp.mean(obs, axis=0)\n",
    "        batch_var = jnp.var(obs, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        return (\n",
    "            (obs - state.mean) / jnp.sqrt(state.var + 1e-8),\n",
    "            state,\n",
    "            reward,\n",
    "            done,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizeVecRewEnvState:\n",
    "    mean: jnp.ndarray\n",
    "    var: jnp.ndarray\n",
    "    count: float\n",
    "    return_val: float\n",
    "    env_state: environment.EnvState\n",
    "\n",
    "\n",
    "class NormalizeVecReward(GymnaxWrapper):\n",
    "    def __init__(self, env, gamma):\n",
    "        super().__init__(env)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        batch_count = obs.shape[0]\n",
    "        state = NormalizeVecRewEnvState(\n",
    "            mean=0.0,\n",
    "            var=1.0,\n",
    "            count=1e-4,\n",
    "            return_val=jnp.zeros((batch_count,)),\n",
    "            env_state=state,\n",
    "        )\n",
    "        return obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "        return_val = state.return_val * self.gamma * (1 - done) + reward\n",
    "\n",
    "        batch_mean = jnp.mean(return_val, axis=0)\n",
    "        batch_var = jnp.var(return_val, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecRewEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            return_val=return_val,\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        return obs, state, reward / jnp.sqrt(state.var + 1e-8), done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "551c8972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 21:38:33.710996: E external/xla/xla/service/slow_operation_alarm.cc:73] Constant folding an instruction is taking > 1s:\n",
      "\n",
      "  %reduce.471 = s32[100,5,1000]{2,1,0} reduce(%constant.15343, %constant.11608), dimensions={2}, to_apply=%region_14.1524.clone, metadata={op_name=\"jit(train)/jit(main)/vmap(jit(reset))/jit(reset)/jit(reset)/jit(calculate_phenotypes)/reduce_sum\" source_file=\"/mnt/c/Users/cltng/gdrive/chewc/chewc/pheno.py\" source_line=33}\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "2025-10-16 21:38:35.427090: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.723403657s\n",
      "Constant folding an instruction is taking > 1s:\n",
      "\n",
      "  %reduce.471 = s32[100,5,1000]{2,1,0} reduce(%constant.15343, %constant.11608), dimensions={2}, to_apply=%region_14.1524.clone, metadata={op_name=\"jit(train)/jit(main)/vmap(jit(reset))/jit(reset)/jit(reset)/jit(calculate_phenotypes)/reduce_sum\" source_file=\"/mnt/c/Users/cltng/gdrive/chewc/chewc/pheno.py\" source_line=33}\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "2025-10-16 21:38:37.702722: E external/xla/xla/service/slow_operation_alarm.cc:73] Constant folding an instruction is taking > 2s:\n",
      "\n",
      "  %reduce.345 = s32[100,5,1000]{2,1,0} reduce(%constant.15608, %constant.10267), dimensions={2}, to_apply=%region_14.1524, metadata={op_name=\"jit(train)/jit(main)/vmap(jit(reset))/jit(reset)/jit(reset)/jit(calculate_phenotypes)/reduce_sum\" source_file=\"/mnt/c/Users/cltng/gdrive/chewc/chewc/pheno.py\" source_line=33}\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "2025-10-16 21:38:38.538468: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.838758233s\n",
      "Constant folding an instruction is taking > 2s:\n",
      "\n",
      "  %reduce.345 = s32[100,5,1000]{2,1,0} reduce(%constant.15608, %constant.10267), dimensions={2}, to_apply=%region_14.1524, metadata={op_name=\"jit(train)/jit(main)/vmap(jit(reset))/jit(reset)/jit(reset)/jit(calculate_phenotypes)/reduce_sum\" source_file=\"/mnt/c/Users/cltng/gdrive/chewc/chewc/pheno.py\" source_line=33}\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "# from wrappers import (\n",
    "#     VecEnv,\n",
    "#     NormalizeVecObservation,\n",
    "#     NormalizeVecReward,\n",
    "#     ClipAction,\n",
    "# )\n",
    "from chewc.gym import StoaEnv\n",
    "\n",
    "#| export\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from gymnax.environments import environment, spaces\n",
    "from typing import Tuple, Optional\n",
    "from functools import partial\n",
    "from jax.tree_util import tree_map\n",
    "\n",
    "# Assuming the 'chewc' library files are in the path\n",
    "from chewc.structs import Population, BreedingState, Trait, GeneticMap, quick_haplo, add_trait\n",
    "from chewc.pheno import calculate_phenotypes\n",
    "from chewc.cross import random_mating, cross_pair\n",
    "\n",
    "class StoaEnv(environment.Environment):\n",
    "    \"\"\"\n",
    "    A Gymnax environment for the ChewC breeding simulation.\n",
    "\n",
    "    The agent's goal is to maximize the genetic gain of the population\n",
    "    over a fixed number of generations by choosing the selection intensity at each step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_founders=100, n_pop_size=200, n_chr=5, n_loci=1000, n_qtl_per_chr=50, total_gen=20, max_crossovers=10):\n",
    "        super().__init__()\n",
    "        self.n_founders = n_founders\n",
    "        self.n_pop_size = n_pop_size\n",
    "        self.total_gen = total_gen\n",
    "        self.max_crossovers = max_crossovers\n",
    "        self.n_chr = n_chr\n",
    "        self.n_loci = n_loci\n",
    "        self.n_qtl_per_chr = n_qtl_per_chr\n",
    "\n",
    "        key = jax.random.PRNGKey(42)\n",
    "        pop_key, trait_key = jax.random.split(key)\n",
    "\n",
    "        # Founder pop is created once and reused for every reset\n",
    "        self.founder_pop, self.genetic_map = quick_haplo(\n",
    "            key=pop_key, n_ind=self.n_founders, n_chr=self.n_chr, seg_sites=self.n_loci\n",
    "        )\n",
    "\n",
    "        self.trait_architecture = add_trait(\n",
    "            key=trait_key,\n",
    "            founder_pop=self.founder_pop,\n",
    "            n_qtl_per_chr=self.n_qtl_per_chr,\n",
    "            mean=jnp.array([100.0]),\n",
    "            var_a=jnp.array([10.0]),\n",
    "            var_d=jnp.array([0.0]),\n",
    "            sigma=jnp.array([[1.0]])\n",
    "        )\n",
    "        self.heritabilities = jnp.array([0.5])\n",
    "\n",
    "    @property\n",
    "    def default_params(self):\n",
    "        return {}\n",
    "\n",
    "    def step_env(self, key: jax.Array, state: BreedingState, action: jax.Array, params) -> Tuple[jax.Array, BreedingState, float, bool, dict]:\n",
    "        key, pheno_key, mating_key, cross_key = jax.random.split(key, 4)\n",
    "        current_pop = state.population\n",
    "\n",
    "        selection_proportion = (action[0] + 1) / 2 * (0.95 - 0.05) + 0.05\n",
    "\n",
    "        phenotypes, _ = calculate_phenotypes(\n",
    "            key=pheno_key,\n",
    "            population=current_pop,\n",
    "            trait=self.trait_architecture,\n",
    "            heritability=self.heritabilities\n",
    "        )\n",
    "\n",
    "        n_current_pop = current_pop.geno.shape[0]\n",
    "        n_select = (n_current_pop * selection_proportion).astype(jnp.int32)\n",
    "        n_select = jnp.maximum(2, n_select)\n",
    "\n",
    "        sorted_indices = jnp.argsort(-phenotypes[:, 0].flatten())\n",
    "        sorted_parent_pool = tree_map(lambda x: x[sorted_indices], current_pop)\n",
    "\n",
    "        pairings = random_mating(mating_key, n_parents=n_select, n_crosses=self.n_pop_size)\n",
    "        mother_indices, father_indices = pairings[:, 0], pairings[:, 1]\n",
    "\n",
    "        mothers_geno = sorted_parent_pool.geno[mother_indices]\n",
    "        fathers_geno = sorted_parent_pool.geno[father_indices]\n",
    "        mothers_ibd = sorted_parent_pool.ibd[mother_indices]\n",
    "        fathers_ibd = sorted_parent_pool.ibd[father_indices]\n",
    "\n",
    "        vmapped_cross = jax.vmap(cross_pair, in_axes=(0, 0, 0, 0, 0, None, None))\n",
    "        offspring_keys = jax.random.split(cross_key, self.n_pop_size)\n",
    "        offspring_geno, offspring_ibd = vmapped_cross(\n",
    "            offspring_keys, mothers_geno, fathers_geno, mothers_ibd, fathers_ibd, self.genetic_map, self.max_crossovers\n",
    "        )\n",
    "\n",
    "        new_generation = state.generation + 1\n",
    "        new_ids = jnp.arange(self.n_pop_size, dtype=jnp.int32) + state.next_id\n",
    "        new_meta = jnp.stack([\n",
    "            new_ids,\n",
    "            sorted_parent_pool.meta[mother_indices, 0],\n",
    "            sorted_parent_pool.meta[father_indices, 0],\n",
    "            jnp.full((self.n_pop_size,), new_generation, dtype=jnp.int32),\n",
    "        ], axis=-1)\n",
    "\n",
    "        new_population = Population(geno=offspring_geno, ibd=offspring_ibd, meta=new_meta)\n",
    "\n",
    "        next_state = BreedingState(\n",
    "            population=new_population, key=key, generation=new_generation, next_id=state.next_id + self.n_pop_size\n",
    "        )\n",
    "\n",
    "        done = next_state.generation >= self.total_gen\n",
    "\n",
    "        def get_final_reward(final_state: BreedingState):\n",
    "            reward_key, _ = jax.random.split(final_state.key)\n",
    "            _, tbvs = calculate_phenotypes(\n",
    "                reward_key, final_state.population, self.trait_architecture, self.heritabilities\n",
    "            )\n",
    "            return jnp.mean(tbvs[:, 0])\n",
    "\n",
    "        reward = jax.lax.cond(done, get_final_reward, lambda s: 0.0, operand=next_state)\n",
    "\n",
    "        obs = self._get_obs(next_state)\n",
    "        return obs, next_state, reward, done, {}\n",
    "\n",
    "    def reset_env(self, key: jax.Array, params) -> Tuple[jax.Array, BreedingState]:\n",
    "        \"\"\"\n",
    "        Resets the environment. Includes a \"burn-in\" step to ensure the\n",
    "        initial population size matches the ongoing population size, which is\n",
    "        required for JAX's conditional logic in auto-resetting environments.\n",
    "        \"\"\"\n",
    "        # Create a new state with the small founder population at generation 0\n",
    "        burn_in_key, pheno_key, mating_key, cross_key = jax.random.split(key, 4)\n",
    "        founder_state = BreedingState(\n",
    "            population=self.founder_pop, key=burn_in_key, generation=0, next_id=self.n_founders\n",
    "        )\n",
    "\n",
    "        # --- Perform a single burn-in breeding step to reach `n_pop_size` ---\n",
    "        phenotypes, _ = calculate_phenotypes(\n",
    "            pheno_key, founder_state.population, self.trait_architecture, self.heritabilities\n",
    "        )\n",
    "\n",
    "        # Select top 50% of founders by default for the first cross\n",
    "        n_select = jnp.maximum(2, self.n_founders // 2)\n",
    "        sorted_indices = jnp.argsort(-phenotypes[:, 0].flatten())\n",
    "        sorted_founders = tree_map(lambda x: x[sorted_indices], founder_state.population)\n",
    "\n",
    "        pairings = random_mating(mating_key, n_parents=n_select, n_crosses=self.n_pop_size)\n",
    "        mother_indices, father_indices = pairings[:, 0], pairings[:, 1]\n",
    "\n",
    "        mothers_geno = sorted_founders.geno[mother_indices]\n",
    "        fathers_geno = sorted_founders.geno[father_indices]\n",
    "        mothers_ibd = sorted_founders.ibd[mother_indices]\n",
    "        fathers_ibd = sorted_founders.ibd[father_indices]\n",
    "\n",
    "        vmapped_cross = jax.vmap(cross_pair, in_axes=(0, 0, 0, 0, 0, None, None))\n",
    "        offspring_keys = jax.random.split(cross_key, self.n_pop_size)\n",
    "        offspring_geno, offspring_ibd = vmapped_cross(\n",
    "            offspring_keys, mothers_geno, fathers_geno, mothers_ibd, fathers_ibd, self.genetic_map, self.max_crossovers\n",
    "        )\n",
    "\n",
    "        # This is the actual initial state for the agent at generation 1\n",
    "        initial_gen = 1\n",
    "        initial_ids = jnp.arange(self.n_pop_size, dtype=jnp.int32) + founder_state.next_id\n",
    "        initial_meta = jnp.stack([\n",
    "            initial_ids,\n",
    "            sorted_founders.meta[mother_indices, 0],\n",
    "            sorted_founders.meta[father_indices, 0],\n",
    "            jnp.full((self.n_pop_size,), initial_gen, dtype=jnp.int32),\n",
    "        ], axis=-1)\n",
    "\n",
    "        initial_pop = Population(geno=offspring_geno, ibd=offspring_ibd, meta=initial_meta)\n",
    "\n",
    "        # The state returned to the agent\n",
    "        initial_state = BreedingState(\n",
    "            population=initial_pop, key=key, generation=initial_gen, next_id=founder_state.next_id + self.n_pop_size\n",
    "        )\n",
    "\n",
    "        obs = self._get_obs(initial_state)\n",
    "        return obs, initial_state\n",
    "\n",
    "    def _get_obs(self, state: BreedingState) -> jax.Array:\n",
    "        remaining_gen = jnp.maximum(0.0, (self.total_gen - state.generation) / self.total_gen)\n",
    "        pheno_key, _ = jax.random.split(state.key)\n",
    "        phenotypes, _ = calculate_phenotypes(\n",
    "            key=pheno_key, population=state.population, trait=self.trait_architecture, heritability=self.heritabilities\n",
    "        )\n",
    "        pheno_trait_1 = phenotypes[:, 0]\n",
    "        genetic_mean = jnp.mean(pheno_trait_1)\n",
    "        genetic_var = jnp.var(pheno_trait_1)\n",
    "        return jnp.array([remaining_gen, genetic_mean, genetic_var])\n",
    "\n",
    "    def observation_space(self, params):\n",
    "        low = jnp.array([0.0, -jnp.inf, 0.0], dtype=jnp.float32)\n",
    "        high = jnp.array([1.0, jnp.inf, jnp.inf], dtype=jnp.float32)\n",
    "        return spaces.Box(low, high, (3,), dtype=jnp.float32)\n",
    "\n",
    "    def action_space(self, params):\n",
    "        return spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=jnp.float32)\n",
    "\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_logtstd = self.param(\"log_std\", nn.initializers.zeros, (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(actor_mean, jnp.exp(actor_logtstd))\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "\n",
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "\n",
    "    # Use your custom environment\n",
    "    env = StoaEnv()\n",
    "    env_params = {} # Your environment doesn't have params\n",
    "\n",
    "    # Add wrappers\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "    env = ClipAction(env)\n",
    "    env = VecEnv(env)\n",
    "    if config[\"NORMALIZE_ENV\"]:\n",
    "        env = NormalizeVecObservation(env)\n",
    "        env = NormalizeVecReward(env, config[\"GAMMA\"])\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "        # INIT NETWORK\n",
    "        network = ActorCritic(\n",
    "            env.action_space(env_params).shape[0], activation=config[\"ACTIVATION\"]\n",
    "        )\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi, value = network.apply(train_state.params, last_obs)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = env.step(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_state, env_state, last_obs, rng = runner_state\n",
    "            _, last_val = network.apply(train_state.params, last_obs)\n",
    "\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of envs\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "            if config.get(\"DEBUG\"):\n",
    "\n",
    "                def callback(info):\n",
    "                    if \"timestep\" in info:\n",
    "                        return_values = info[\"returned_episode_returns\"][\n",
    "                            info[\"returned_episode\"]\n",
    "                        ]\n",
    "                        timesteps = (\n",
    "                            info[\"timestep\"][info[\"returned_episode\"]] * config[\"NUM_ENVS\"]\n",
    "                        )\n",
    "                        for t in range(len(timesteps)):\n",
    "                            print(\n",
    "                                f\"global step={timesteps[t]}, episodic return={return_values[t]}\"\n",
    "                            )\n",
    "\n",
    "                jax.debug.callback(callback, metric)\n",
    "\n",
    "            runner_state = (train_state, env_state, last_obs, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"LR\": 3e-4,\n",
    "        \"NUM_ENVS\": 1,          # Increased for more parallel data collection on the GPU\n",
    "        \"NUM_STEPS\": 2048,         # More steps per update to better handle sparse rewards\n",
    "        \"TOTAL_TIMESTEPS\": 5e6,    # A much longer run to see long-term learning\n",
    "        \"UPDATE_EPOCHS\": 4,\n",
    "        \"NUM_MINIBATCHES\": 32,     # Adjusted for the new, larger batch size\n",
    "        \"GAMMA\": 0.99,\n",
    "        \"GAE_LAMBDA\": 0.95,\n",
    "        \"CLIP_EPS\": 0.2,\n",
    "        \"ENT_COEF\": 0.0,\n",
    "        \"VF_COEF\": 0.5,\n",
    "        \"MAX_GRAD_NORM\": 0.5,\n",
    "        \"ACTIVATION\": \"tanh\",\n",
    "        \"ANNEAL_LR\": True,\n",
    "        \"NORMALIZE_ENV\": True,\n",
    "        \"DEBUG\": False,            # Disabled for a long run to keep the output clean\n",
    "    }\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    train_jit = jax.jit(make_train(config))\n",
    "    out = train_jit(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea731427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['metrics', 'runner_state'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebca2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# This assumes 'out' is the dictionary returned by your train_jit function\n",
    "metrics = out[\"metrics\"]\n",
    "\n",
    "# Filter out the returns for only the episodes that have actually finished\n",
    "episode_returns = metrics[\"returned_episode_returns\"][metrics[\"returned_episode\"]]\n",
    "\n",
    "# Create a plot of the episodic returns\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(episode_returns, alpha=0.6, label='Raw Episodic Return')\n",
    "\n",
    "# Calculate and plot a rolling average to see the trend more clearly\n",
    "# A window of 20 episodes is a good starting point\n",
    "if len(episode_returns) > 20:\n",
    "    rolling_avg = pd.Series(episode_returns).rolling(window=20, min_periods=1).mean()\n",
    "    plt.plot(rolling_avg, label='Rolling Average (20 episodes)', linewidth=2, color='C1')\n",
    "\n",
    "plt.title(\"Agent's Learning Progress\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Total Return per Episode\")\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# You can display the plot in your notebook with plt.show()\n",
    "# or save it to a file\n",
    "# plt.savefig('training_progress.png')\n",
    "# print(\"Plot saved as training_progress.png\")\n",
    "# plt.show() # Uncomment this line if you are in a Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98282b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
