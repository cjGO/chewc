{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict\n",
    "\n",
    "> Common operations around the core datastructures for running a sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from flax.struct import dataclass as flax_dataclass\n",
    "import jax.numpy as jnp\n",
    "from typing import Optional\n",
    "from chewc.population import Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\"\"\"\n",
    "GBLUP (Genomic Best Linear Unbiased Prediction) implementation for chewc library.\n",
    "\n",
    "This module provides functions for genomic prediction using the GBLUP methodology,\n",
    "which is a standard approach in genomic selection and animal breeding.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Optional, Dict\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.numpy.linalg import solve, inv, pinv\n",
    "from flax.struct import dataclass as flax_dataclass\n",
    "\n",
    "from chewc.population import Population\n",
    "\n",
    "@flax_dataclass(frozen=True)\n",
    "class PredictionResults:\n",
    "    \"\"\"\n",
    "    A container for the results of a genomic prediction.\n",
    "    \"\"\"\n",
    "    ids: jnp.ndarray\n",
    "    ebv: jnp.ndarray\n",
    "    pev: Optional[jnp.ndarray] = None\n",
    "    reliability: Optional[jnp.ndarray] = None\n",
    "    fixed_effects: Optional[jnp.ndarray] = None\n",
    "    h2_used: Optional[float] = None\n",
    "    var_components: Optional[Dict] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from functools import partial\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Literal, Dict, Optional\n",
    "from flax.struct import dataclass as flax_dataclass\n",
    "from jax.scipy.sparse.linalg import cg\n",
    "import copy\n",
    "\n",
    "# --- Import all necessary functions from your codebase ---\n",
    "# Make sure these paths are correct for your project structure\n",
    "from chewc.population import Population, msprime_pop, combine_populations\n",
    "from chewc.sp import SimParam\n",
    "from chewc.trait import add_trait_a\n",
    "from chewc.phenotype import set_bv, set_pheno\n",
    "from chewc.cross import make_cross\n",
    "\n",
    "@partial(jax.jit, static_argnames=('n_ind',))\n",
    "def _jit_calc_a_inverse(mother_iids: jnp.ndarray, father_iids: jnp.ndarray, n_ind: int) -> jnp.ndarray:\n",
    "    initial_A_inv = jnp.zeros((n_ind, n_ind))\n",
    "    def loop_body(i, A_inv):\n",
    "        sire_iid, dam_iid = father_iids[i], mother_iids[i]\n",
    "        case_index = (sire_iid != -1) + 2 * (dam_iid != -1)\n",
    "        def case_0(mat): return mat.at[i, i].add(1.0)\n",
    "        def case_1(mat): return mat.at[i, i].add(4/3.).at[sire_iid, sire_iid].add(1/3.).at[i, sire_iid].add(-2/3.).at[sire_iid, i].add(-2/3.)\n",
    "        def case_2(mat): return mat.at[i, i].add(4/3.).at[dam_iid, dam_iid].add(1/3.).at[i, dam_iid].add(-2/3.).at[dam_iid, i].add(-2/3.)\n",
    "        def case_3(mat): return (mat.at[i, i].add(2.0).at[sire_iid, sire_iid].add(0.5).at[dam_iid, dam_iid].add(0.5)\n",
    "                                .at[sire_iid, dam_iid].add(0.5).at[dam_iid, sire_iid].add(0.5).at[i, sire_iid].add(-1.0)\n",
    "                                .at[sire_iid, i].add(-1.0).at[i, dam_iid].add(-1.0).at[dam_iid, i].add(-1.0))\n",
    "        return lax.switch(case_index, [case_0, case_1, case_2, case_3], A_inv)\n",
    "    return lax.fori_loop(0, n_ind, loop_body, initial_A_inv)\n",
    "\n",
    "def calc_a_inverse_matrix_pedigree_jax(pop: Population) -> jnp.ndarray:\n",
    "    n_ind = pop.nInd\n",
    "    id_to_iid_map = jnp.full(pop.id.max() + 2, -1, dtype=jnp.int32).at[pop.id].set(pop.iid)\n",
    "    mother_iids = jnp.where(pop.mother < 0, -1, id_to_iid_map[pop.mother.clip(min=0)])\n",
    "    father_iids = jnp.where(pop.father < 0, -1, id_to_iid_map[pop.father.clip(min=0)])\n",
    "    return _jit_calc_a_inverse(mother_iids, father_iids, n_ind)\n",
    "\n",
    "@jax.jit\n",
    "def calc_g_matrix(geno_dosage: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the genomic relationship matrix (G).\n",
    "    --- FIX ---\n",
    "    The original epsilon was too small to prevent the G matrix from becoming\n",
    "    ill-conditioned in later generations. Increasing it slightly adds a larger\n",
    "    \"ridge\" to the diagonal, making the matrix more stable and invertible.\n",
    "    We also blend G with the identity matrix, which is a common technique to\n",
    "    improve the conditioning of G.\n",
    "    \"\"\"\n",
    "    n_ind, n_markers = geno_dosage.shape\n",
    "    p = jnp.mean(geno_dosage, axis=0) / 2.0\n",
    "    P = 2 * p\n",
    "    M = geno_dosage - P\n",
    "    denominator = 2 * jnp.sum(p * (1 - p))\n",
    "    denominator = jnp.where(denominator == 0, 1e-8, denominator)\n",
    "    G_raw = (M @ M.T) / denominator\n",
    "\n",
    "    # --- FIX ---\n",
    "    # Blend G with the identity matrix to improve conditioning.\n",
    "    # 0.99 is a common blending factor.\n",
    "    G = 0.99 * G_raw + 0.01 * jnp.identity(n_ind)\n",
    "    \n",
    "    # Add a slightly larger epsilon for numerical stability.\n",
    "    epsilon = 1e-3\n",
    "    return G + jnp.identity(n_ind) * epsilon\n",
    "\n",
    "@partial(jax.jit, static_argnames='n_ind')\n",
    "def _mme_solver_cg(pheno: jnp.ndarray, train_mask: jnp.ndarray, K_inv: jnp.ndarray, h2: float, n_ind: int) -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Mixed model equation (MME) solver using the conjugate gradient algorithm.\n",
    "    --- FIX ---\n",
    "    The preconditioner is improved by adding a small offset to the diagonal elements.\n",
    "    This prevents division by very small numbers if any diagonal elements of K_inv\n",
    "    are close to zero, which can happen with ill-conditioned matrices.\n",
    "    \"\"\"\n",
    "    alpha = (1.0 - h2) / h2\n",
    "    y = jnp.nan_to_num(pheno.flatten())\n",
    "    train_mask_float = train_mask.astype(jnp.float32)\n",
    "    n_train = jnp.sum(train_mask_float)\n",
    "\n",
    "    def lhs_matvec(solution_vector):\n",
    "        beta, u = solution_vector[0], solution_vector[1:]\n",
    "        row1 = n_train * beta + jnp.sum(u * train_mask_float)\n",
    "        row2 = train_mask_float * beta + u * train_mask_float + (K_inv @ u) * alpha\n",
    "        return jnp.concatenate([jnp.array([row1]), row2])\n",
    "\n",
    "    rhs = jnp.concatenate([\n",
    "        jnp.array([jnp.sum(y * train_mask_float)]),\n",
    "        y * train_mask_float\n",
    "    ])\n",
    "\n",
    "    # --- FIX ---\n",
    "    # A more robust diagonal preconditioner. Adding a small constant (1e-6)\n",
    "    # ensures that we don't divide by zero, even if some diagonal\n",
    "    # elements are very small.\n",
    "    M_diag = jnp.concatenate([\n",
    "        jnp.array([n_train]),\n",
    "        train_mask_float + jnp.diag(K_inv) * alpha + 1e-6\n",
    "    ])\n",
    "    M_diag = jnp.maximum(M_diag, 1e-6)\n",
    "    \n",
    "    # Increase the tolerance and max iterations for the CG solver for more stability\n",
    "    solutions, _ = cg(lhs_matvec, rhs, M=lambda x: x / M_diag, tol=1e-5, maxiter=2*n_ind)\n",
    "    return solutions[0:1], solutions[1:]\n",
    "\n",
    "# ==================================================\n",
    "# --- Prediction Functions (ABLUP and GBLUP) ---\n",
    "# ==================================================\n",
    "def mme_predict_ablup(pop: Population, h2: float, trait_idx: int = 0) -> PredictionResults:\n",
    "    pheno = pop.pheno[:, trait_idx:trait_idx+1]\n",
    "    n_ind = pop.nInd\n",
    "    train_mask = ~jnp.isnan(pheno.flatten())\n",
    "    if jnp.sum(train_mask) == 0: raise ValueError(\"No individuals with phenotypes.\")\n",
    "    K_inv = calc_a_inverse_matrix_pedigree_jax(pop)\n",
    "    \n",
    "    # --- FIX ---\n",
    "    # Call the unified MME solver with the inverse of the pedigree relationship matrix (A_inv).\n",
    "    fixed_effects, all_ebv = _mme_solver_cg(pheno, train_mask, K_inv, h2, n_ind)\n",
    "    return PredictionResults(ids=pop.id, ebv=all_ebv.reshape(-1, 1), fixed_effects=fixed_effects, h2_used=h2)\n",
    "\n",
    "def mme_predict_gblup(pop: Population, h2: float, trait_idx: int = 0) -> PredictionResults:\n",
    "    \"\"\"\n",
    "    Predicts Estimated Breeding Values (EBVs) using Genomic Best Linear Unbiased Prediction (GBLUP).\n",
    "    This version includes jax.debug.print statements to diagnose NaN issues.\n",
    "    \"\"\"\n",
    "    pheno = pop.pheno[:, trait_idx:trait_idx+1]\n",
    "    n_ind = pop.nInd\n",
    "    train_mask = ~jnp.isnan(pheno.flatten())\n",
    "\n",
    "    if jnp.sum(train_mask) == 0:\n",
    "        raise ValueError(\"No individuals with phenotypes.\")\n",
    "\n",
    "    # --- JAX DEBUG STATEMENTS ---\n",
    "    jax.debug.print(\"--- GBLUP Debug Start ---\")\n",
    "    jax.debug.print(\"h2: {h2}\", h2=h2)\n",
    "    jax.debug.print(\"Number of individuals: {n}\", n=n_ind)\n",
    "    jax.debug.print(\"Number of phenotyped individuals: {p}\", p=jnp.sum(train_mask))\n",
    "    jax.debug.print(\"Pheno shape: {s}, contains NaNs: {n}\", s=pheno.shape, n=jnp.any(jnp.isnan(pheno)))\n",
    "    # --- END DEBUG ---\n",
    "\n",
    "    G = calc_g_matrix(pop.dosage)\n",
    "\n",
    "    # --- JAX DEBUG STATEMENTS ---\n",
    "    jax.debug.print(\"G matrix shape: {s}\", s=G.shape)\n",
    "    jax.debug.print(\"G matrix contains NaNs: {n}\", n=jnp.any(jnp.isnan(G)))\n",
    "    jax.debug.print(\"G matrix diagonal min/max: {min}/{max}\", min=jnp.min(jnp.diag(G)), max=jnp.max(jnp.diag(G)))\n",
    "    # Check the condition number to assess if G is ill-conditioned (close to singular)\n",
    "    cond_g = jnp.linalg.cond(G)\n",
    "    jax.debug.print(\"G matrix condition number: {c}\", c=cond_g)\n",
    "    # --- END DEBUG ---\n",
    "\n",
    "    # Invert the G matrix. A large condition number might cause NaNs here.\n",
    "    G_inv = jnp.linalg.inv(G)\n",
    "\n",
    "    # --- JAX DEBUG STATEMENTS ---\n",
    "    jax.debug.print(\"G_inv matrix contains NaNs: {n}\", n=jnp.any(jnp.isnan(G_inv)))\n",
    "    # --- END DEBUG ---\n",
    "\n",
    "    fixed_effects, all_ebv = _mme_solver_cg(pheno, train_mask, G_inv, h2, n_ind)\n",
    "\n",
    "    # --- JAX DEBUG STATEMENTS ---\n",
    "    jax.debug.print(\"Solved fixed_effects contains NaNs: {n}\", n=jnp.any(jnp.isnan(fixed_effects)))\n",
    "    jax.debug.print(\"Solved all_ebv contains NaNs: {n}\", n=jnp.any(jnp.isnan(all_ebv)))\n",
    "    jax.debug.print(\"--- GBLUP Debug End ---\")\n",
    "    # --- END DEBUG ---\n",
    "\n",
    "    return PredictionResults(ids=pop.id, ebv=all_ebv.reshape(-1, 1), fixed_effects=fixed_effects, h2_used=h2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from functools import partial\n",
    "from jax.scipy.sparse.linalg import cg\n",
    "from typing import Tuple\n",
    "\n",
    "# Assume Population and PaddedPopulation are importable\n",
    "# from .population import Population, PaddedPopulation\n",
    "# from .predict import PredictionResults\n",
    "\n",
    "@partial(jax.jit, static_argnames=('max_size',))\n",
    "def _jit_calc_a_inverse_padded(\n",
    "    mother_iids: jnp.ndarray,\n",
    "    father_iids: jnp.ndarray,\n",
    "    is_active: jnp.ndarray,\n",
    "    max_size: int\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    (JIT-compiled) Calculates A-inverse for a padded population,\n",
    "    skipping inactive individuals.\n",
    "    \"\"\"\n",
    "    initial_A_inv = jnp.zeros((max_size, max_size))\n",
    "\n",
    "    def loop_body(i, A_inv):\n",
    "        # This function applies Henderson's rules if the individual is active\n",
    "        def apply_rules(mat):\n",
    "            sire_iid, dam_iid = father_iids[i], mother_iids[i]\n",
    "            case_index = (sire_iid != -1) + 2 * (dam_iid != -1)\n",
    "            \n",
    "            def case_0(m): return m.at[i, i].add(1.0)\n",
    "            def case_1(m): return m.at[i, i].add(4/3.).at[sire_iid, sire_iid].add(1/3.).at[i, sire_iid].add(-2/3.).at[sire_iid, i].add(-2/3.)\n",
    "            def case_2(m): return m.at[i, i].add(4/3.).at[dam_iid, dam_iid].add(1/3.).at[i, dam_iid].add(-2/3.).at[dam_iid, i].add(-2/3.)\n",
    "            def case_3(m): return (m.at[i, i].add(2.0).at[sire_iid, sire_iid].add(0.5).at[dam_iid, dam_iid].add(0.5)\n",
    "                                    .at[sire_iid, dam_iid].add(0.5).at[dam_iid, sire_iid].add(0.5).at[i, sire_iid].add(-1.0)\n",
    "                                    .at[sire_iid, i].add(-1.0).at[i, dam_iid].add(-1.0).at[dam_iid, i].add(-1.0))\n",
    "            \n",
    "            return lax.switch(case_index, [case_0, case_1, case_2, case_3], mat)\n",
    "\n",
    "        # This function does nothing if the individual is inactive\n",
    "        def do_nothing(mat):\n",
    "            return mat\n",
    "\n",
    "        # Use lax.cond to choose which function to apply\n",
    "        return lax.cond(is_active[i], apply_rules, do_nothing, A_inv)\n",
    "\n",
    "    return lax.fori_loop(0, max_size, loop_body, initial_A_inv)\n",
    "\n",
    "def calc_a_inverse_matrix_pedigree_jax_padded(pop: \"PaddedPopulation\") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Wrapper to calculate the inverse A-matrix for a PaddedPopulation.\n",
    "    This version correctly handles the padded structure and is JIT-compatible.\n",
    "    \"\"\"\n",
    "    max_size = pop.max_size\n",
    "    \n",
    "    # The id_to_iid_map needs to be large enough to handle all possible IDs.\n",
    "    # We find the maximum ID among the *active* individuals.\n",
    "    max_id = jnp.max(jnp.where(pop.is_active, pop.id, -1))\n",
    "    \n",
    "    # The map size should be max_id + 2, but at least `max_size` to avoid issues\n",
    "    # if new IDs are much larger than the number of individuals.\n",
    "    map_size = jnp.maximum(max_id + 2, max_size)\n",
    "\n",
    "    id_to_iid_map = jnp.full(map_size, -1, dtype=jnp.int32).at[pop.id].set(pop.iid)\n",
    "\n",
    "    # Map parent public IDs to internal iids. Inactive individuals will have\n",
    "    # parents mapped to -1, which is handled correctly.\n",
    "    mother_iids = jnp.where(pop.mother < 0, -1, id_to_iid_map[pop.mother.clip(min=0)])\n",
    "    father_iids = jnp.where(pop.father < 0, -1, id_to_iid_map[pop.father.clip(min=0)])\n",
    "    \n",
    "    return _jit_calc_a_inverse_padded(mother_iids, father_iids, pop.is_active, max_size)\n",
    "\n",
    "@jax.jit\n",
    "def calc_g_matrix_padded(pop: \"PaddedPopulation\") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the genomic relationship matrix (G) for a PaddedPopulation.\n",
    "    Allele frequencies are calculated based *only* on active individuals.\n",
    "    \"\"\"\n",
    "    max_size, n_markers = pop.dosage.shape\n",
    "    \n",
    "    # Mask the dosage to only include active individuals for allele freq calculation\n",
    "    active_dosage = jnp.where(pop.is_active[:, None], pop.dosage, 0)\n",
    "    \n",
    "    # Sum of dosages for active individuals\n",
    "    sum_dosage = jnp.sum(active_dosage, axis=0)\n",
    "    \n",
    "    # Calculate allele frequencies `p` using the true number of active individuals\n",
    "    p = (sum_dosage / (2 * pop.n_ind))\n",
    "    \n",
    "    # Center the full (padded) dosage matrix\n",
    "    P = 2 * p\n",
    "    M = pop.dosage - P\n",
    "    \n",
    "    # Mask the centered matrix so inactive individuals do not contribute to G\n",
    "    M_masked = jnp.where(pop.is_active[:, None], M, 0)\n",
    "    \n",
    "    denominator = 2 * jnp.sum(p * (1 - p))\n",
    "    denominator = jnp.where(denominator == 0, 1e-8, denominator)\n",
    "    G_raw = (M_masked @ M_masked.T) / denominator\n",
    "\n",
    "    # Blend and regularize, ensuring the identity matrix matches the padded size\n",
    "    G = 0.99 * G_raw + 0.01 * jnp.identity(max_size)\n",
    "    epsilon = 1e-3\n",
    "    return G + jnp.identity(max_size) * epsilon\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames='max_size')\n",
    "def _mme_solver_cg_padded(\n",
    "    pheno: jnp.ndarray,\n",
    "    train_mask: jnp.ndarray,\n",
    "    K_inv: jnp.ndarray,\n",
    "    h2: float,\n",
    "    max_size: int\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    (JIT-compiled) MME solver using conjugate gradient for padded arrays.\n",
    "    This function is almost identical to the original but uses `max_size`.\n",
    "    \"\"\"\n",
    "    alpha = (1.0 - h2) / h2\n",
    "    y = jnp.nan_to_num(pheno.flatten())\n",
    "    train_mask_float = train_mask.astype(jnp.float32)\n",
    "    n_train = jnp.sum(train_mask_float)\n",
    "\n",
    "    def lhs_matvec(solution_vector):\n",
    "        beta, u = solution_vector[0], solution_vector[1:]\n",
    "        row1 = n_train * beta + jnp.sum(u * train_mask_float)\n",
    "        row2 = train_mask_float * beta + u * train_mask_float + (K_inv @ u) * alpha\n",
    "        return jnp.concatenate([jnp.array([row1]), row2])\n",
    "\n",
    "    rhs = jnp.concatenate([\n",
    "        jnp.array([jnp.sum(y * train_mask_float)]),\n",
    "        y * train_mask_float\n",
    "    ])\n",
    "\n",
    "    M_diag = jnp.concatenate([\n",
    "        jnp.array([n_train]),\n",
    "        train_mask_float + jnp.diag(K_inv) * alpha + 1e-6\n",
    "    ])\n",
    "    M_diag = jnp.maximum(M_diag, 1e-6)\n",
    "    \n",
    "    # Use max_size for the maxiter argument for static shape compatibility\n",
    "    solutions, _ = cg(lhs_matvec, rhs, M=lambda x: x / M_diag, tol=1e-5, maxiter=2 * max_size)\n",
    "    return solutions[0:1], solutions[1:]\n",
    "\n",
    "def mme_predict_ablup_padded(pop: \"PaddedPopulation\", h2: float, trait_idx: int = 0) -> \"PredictionResults\":\n",
    "    \"\"\"Padded-aware ABLUP prediction.\"\"\"\n",
    "    pheno = pop.pheno[:, trait_idx:trait_idx+1]\n",
    "    max_size = pop.max_size\n",
    "    \n",
    "    # The train_mask must account for BOTH active status and having a phenotype\n",
    "    train_mask = pop.is_active & ~jnp.isnan(pheno.flatten())\n",
    "    \n",
    "    # JAX does not raise errors inside JIT, so a host-side check is useful\n",
    "    if jnp.sum(train_mask) == 0:\n",
    "        raise ValueError(\"No individuals with phenotypes found among active population.\")\n",
    "        \n",
    "    K_inv = calc_a_inverse_matrix_pedigree_jax_padded(pop)\n",
    "    \n",
    "    fixed_effects, all_ebv = _mme_solver_cg_padded(pheno, train_mask, K_inv, h2, max_size)\n",
    "    \n",
    "    # The result contains predictions for all max_size slots.\n",
    "    # Downstream functions should use the `is_active` mask to select relevant results.\n",
    "    return PredictionResults(ids=pop.id, ebv=all_ebv.reshape(-1, 1), fixed_effects=fixed_effects, h2_used=h2)\n",
    "\n",
    "def mme_predict_gblup_padded(pop: \"PaddedPopulation\", h2: float, trait_idx: int = 0) -> \"PredictionResults\":\n",
    "    \"\"\"Padded-aware GBLUP prediction.\"\"\"\n",
    "    pheno = pop.pheno[:, trait_idx:trait_idx+1]\n",
    "    max_size = pop.max_size\n",
    "\n",
    "    # The train_mask must account for BOTH active status and having a phenotype\n",
    "    train_mask = pop.is_active & ~jnp.isnan(pheno.flatten())\n",
    "\n",
    "    if jnp.sum(train_mask) == 0:\n",
    "        raise ValueError(\"No individuals with phenotypes found among active population.\")\n",
    "\n",
    "    G = calc_g_matrix_padded(pop)\n",
    "    \n",
    "    # Add a small value to the diagonal for stability before inverting.\n",
    "    # This is often more stable than inverting and then adding a ridge.\n",
    "    G_inv = jnp.linalg.inv(G + jnp.identity(max_size) * 1e-4)\n",
    "\n",
    "    fixed_effects, all_ebv = _mme_solver_cg_padded(pheno, train_mask, G_inv, h2, max_size)\n",
    "    \n",
    "    return PredictionResults(ids=pop.id, ebv=all_ebv.reshape(-1, 1), fixed_effects=fixed_effects, h2_used=h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
