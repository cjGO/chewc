{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7461920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2449aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd7924b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca870b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 276) (3923746354.py, line 276)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 276\u001b[0;36m\u001b[0m\n\u001b[0;31m    display(step_wise_df)eline Performance')\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 276)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from chewc.gym import StoaEnv\n",
    "from chewc.pheno import calculate_phenotypes\n",
    "\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "\n",
    "def run_constant_action_episode(env: StoaEnv, action_value: float, rng_key: jax.Array):\n",
    "    params = env.default_params\n",
    "    reset_key, rng_key = jax.random.split(rng_key)\n",
    "    _, state = env.reset_env(reset_key, params)\n",
    "\n",
    "    action = jnp.array([action_value], dtype=jnp.float32)\n",
    "    final_reward = 0.0\n",
    "\n",
    "    while True:\n",
    "        rng_key, step_key = jax.random.split(rng_key)\n",
    "        _, state, reward, done, _ = env.step_env(step_key, state, action, params)\n",
    "        final_reward = float(reward)\n",
    "        if bool(done):\n",
    "            break\n",
    "\n",
    "    phenokey, _ = jax.random.split(state.key)\n",
    "    phenotypes, tbv = calculate_phenotypes(\n",
    "        phenokey,\n",
    "        population=state.population,\n",
    "        trait=env.trait_architecture,\n",
    "        heritability=env.heritabilities,\n",
    "    )\n",
    "\n",
    "    phenotypes = np.asarray(phenotypes[:, 0])\n",
    "    tbv = np.asarray(tbv[:, 0])\n",
    "    return final_reward, float(phenotypes.mean()), float(tbv.mean())\n",
    "\n",
    "\n",
    "def evaluate_constant_actions(\n",
    "    env: StoaEnv,\n",
    "    action_values,\n",
    "    num_episodes: int = 32,\n",
    "    seed: int = 0,\n",
    ") -> pd.DataFrame:\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    records = []\n",
    "\n",
    "    for action in action_values:\n",
    "        rewards = []\n",
    "        phenotype_means = []\n",
    "        tbv_means = []\n",
    "\n",
    "        for _ in range(num_episodes):\n",
    "            rng, episode_key = jax.random.split(rng)\n",
    "            reward, phenotype_mean, tbv_mean = run_constant_action_episode(\n",
    "                env, action, episode_key\n",
    "            )\n",
    "            rewards.append(reward)\n",
    "            phenotype_means.append(phenotype_mean)\n",
    "            tbv_means.append(tbv_mean)\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                'action': float(action),\n",
    "                'reward_mean': float(np.mean(rewards)),\n",
    "                'reward_std': float(np.std(rewards, ddof=1)) if num_episodes > 1 else 0.0,\n",
    "                'phenotype_mean': float(np.mean(phenotype_means)),\n",
    "                'phenotype_std': float(np.std(phenotype_means, ddof=1)) if num_episodes > 1 else 0.0,\n",
    "                'tbv_mean': float(np.mean(tbv_means)),\n",
    "                'tbv_std': float(np.std(tbv_means, ddof=1)) if num_episodes > 1 else 0.0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "gym = StoaEnv(total_gen=5)\n",
    "\n",
    "baseline_actions = [-1.0, -0.5]\n",
    "baseline_df = evaluate_constant_actions(gym, baseline_actions, num_episodes=5, seed=0)\n",
    "display(baseline_df)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharex=True)\n",
    "metrics = [\n",
    "    ('reward_mean', 'Reward'),\n",
    "    ('phenotype_mean', 'Phenotype Mean'),\n",
    "    ('tbv_mean', 'TBV Mean'),\n",
    "]\n",
    "\n",
    "for ax, (column, title) in zip(axes, metrics):\n",
    "    ax.bar(baseline_df['action'], baseline_df[column], color='#4c72b0')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Action value')\n",
    "    ax.set_xticks(baseline_df['action'])\n",
    "    ax.set_ylabel(title)\n",
    "    ax.grid(axis='y', alpha=0.2, linestyle='--', linewidth=0.7)\n",
    "\n",
    "fig.suptitle('Constant Action Bas')\n",
    "def run_constant_action_episode(env: StoaEnv, action_value: float, rng_key: jax.Array):\n",
    "    \"\"\"Runs a single episode with a constant action and tracks step-wise data.\"\"\"\n",
    "    params = env.default_params\n",
    "    reset_key, rng_key = jax.random.split(rng_key)\n",
    "    _, state = env.reset_env(reset_key, params)\n",
    "\n",
    "    action = jnp.array([action_value], dtype=jnp.float32)\n",
    "\n",
    "    # Lists to store step-wise data\n",
    "    step_rewards = []\n",
    "    step_phenotype_means = []\n",
    "    step_tbv_means = []\n",
    "    step_actions = [] # Store action at each step\n",
    "    steps = []       # Store step number\n",
    "\n",
    "    current_step = 0\n",
    "    while True:\n",
    "        rng_key, step_key = jax.random.split(rng_key)\n",
    "        phenokey, rng_key = jax.random.split(rng_key) # Need a key for phenotype calculation each step\n",
    "\n",
    "        # Calculate metrics *before* taking the step to align with step number\n",
    "        current_phenotypes, current_tbv = calculate_phenotypes(\n",
    "            phenokey,\n",
    "            population=state.population,\n",
    "            trait=env.trait_architecture,\n",
    "            heritability=env.heritabilities,\n",
    "        )\n",
    "        step_phenotype_means.append(float(jnp.asarray(current_phenotypes[:, 0]).mean()))\n",
    "        step_tbv_means.append(float(jnp.asarray(current_tbv[:, 0]).mean()))\n",
    "        step_actions.append(action_value) # Store the constant action\n",
    "        steps.append(current_step)\n",
    "\n",
    "        # Environment step\n",
    "        _, state, reward, done, _ = env.step_env(step_key, state, action, params)\n",
    "        step_rewards.append(float(reward)) # Store reward received *after* the step\n",
    "\n",
    "        current_step += 1\n",
    "\n",
    "        if bool(done):\n",
    "            # Record final state metrics if needed (or adjust logic based on when metrics should reflect state)\n",
    "            # For simplicity here, we stop after the 'done' flag is True\n",
    "            break\n",
    "\n",
    "    # Final metrics calculation (optional, depending on what 'final_reward' represents)\n",
    "    # phenokey_final, _ = jax.random.split(state.key)\n",
    "    # phenotypes_final, tbv_final = calculate_phenotypes(\n",
    "    #     phenokey_final,\n",
    "    #     population=state.population,\n",
    "    #     trait=env.trait_architecture,\n",
    "    #     heritability=env.heritabilities,\n",
    "    # )\n",
    "    # final_phenotype_mean = float(jnp.asarray(phenotypes_final[:, 0]).mean())\n",
    "    # final_tbv_mean = float(jnp.asarray(tbv_final[:, 0]).mean())\n",
    "    final_reward = step_rewards[-1] # Example: use the last reward\n",
    "\n",
    "    # Return step-wise data along with final metrics\n",
    "    step_data = pd.DataFrame({\n",
    "        'step': steps,\n",
    "        'action': step_actions,\n",
    "        'reward': step_rewards, # Note: reward list might be one shorter if not recorded after final step before break\n",
    "        'phenotype_mean': step_phenotype_means,\n",
    "        'tbv_mean': step_tbv_means\n",
    "    })\n",
    "\n",
    "    # Adjust reward list length if necessary\n",
    "    if len(step_rewards) < len(steps):\n",
    "         step_data = step_data.iloc[:-1].copy() # Drop last row if reward list is shorter\n",
    "         step_data['reward'] = step_rewards\n",
    "\n",
    "\n",
    "    return final_reward, step_data # Return final reward and the DataFrame of step data\n",
    "\n",
    "\n",
    "def evaluate_constant_actions(\n",
    "    env: StoaEnv,\n",
    "    action_values,\n",
    "    num_episodes: int = 32,\n",
    "    seed: int = 0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Evaluates constant actions, collecting step-wise data for the first episode of each action.\"\"\"\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    # records = [] # Keeping this if you still want summary stats\n",
    "    all_step_data = [] # List to store step data DataFrames\n",
    "\n",
    "    for action in action_values:\n",
    "        rewards = []\n",
    "        # phenotype_means = [] # For summary stats if needed\n",
    "        # tbv_means = [] # For summary stats if needed\n",
    "        first_episode_step_data = None\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            rng, episode_key = jax.random.split(rng)\n",
    "            final_reward, step_data = run_constant_action_episode(\n",
    "                env, action, episode_key\n",
    "            )\n",
    "            rewards.append(final_reward)\n",
    "            # phenotype_means.append(step_data['phenotype_mean'].iloc[-1]) # Example: final phenotype mean\n",
    "            # tbv_means.append(step_data['tbv_mean'].iloc[-1]) # Example: final tbv mean\n",
    "\n",
    "            if i == 0: # Store step data only for the first episode\n",
    "                step_data['action_value'] = float(action) # Add action value column for grouping/labeling\n",
    "                step_data['episode'] = i # Add episode identifier\n",
    "                first_episode_step_data = step_data\n",
    "\n",
    "        all_step_data.append(first_episode_step_data)\n",
    "\n",
    "        # If you still need the summary DataFrame, uncomment and adjust this part\n",
    "        # records.append(\n",
    "        #     {\n",
    "        #         'action': float(action),\n",
    "        #         'reward_mean': float(np.mean(rewards)),\n",
    "        #         'reward_std': float(np.std(rewards, ddof=1)) if num_episodes > 1 else 0.0,\n",
    "        #         'phenotype_mean': float(np.mean(phenotype_means)),\n",
    "        #         'phenotype_std': float(np.std(phenotype_means, ddof=1)) if num_episodes > 1 else 0.0,\n",
    "        #         'tbv_mean': float(np.mean(tbv_means)),\n",
    "        #         'tbv_std': float(np.std(tbv_means, ddof=1)) if num_episodes > 1 else 0.0,\n",
    "        #     }\n",
    "        # )\n",
    "\n",
    "    # Concatenate step data from the first episode of each action\n",
    "    combined_step_data = pd.concat(all_step_data, ignore_index=True)\n",
    "\n",
    "    # return pd.DataFrame.from_records(records) # Return summary if needed\n",
    "    return combined_step_data\n",
    "\n",
    "\n",
    "# --- Simulation & Plotting ---\n",
    "gym = StoaEnv(total_gen=10) # Increase generations for a longer plot\n",
    "\n",
    "baseline_actions = [-1.0, -0.5, 0.0, 0.5, 1.0] # Evaluate more actions\n",
    "step_wise_df = evaluate_constant_actions(gym, baseline_actions, num_episodes=5, seed=0) # Run simulation\n",
    "\n",
    "# --- New Plotting Logic ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True) # Changed to 2x2 grid\n",
    "axes = axes.flatten() # Flatten axes array for easy iteration\n",
    "\n",
    "metrics_to_plot = [\n",
    "    ('action', 'Action Taken'),\n",
    "    ('reward', 'Step Reward'),\n",
    "    ('phenotype_mean', 'Phenotype Mean'),\n",
    "    ('tbv_mean', 'TBV Mean'),\n",
    "]\n",
    "\n",
    "# Group data by the constant action value used for the episode\n",
    "grouped_data = step_wise_df.groupby('action_value')\n",
    "\n",
    "# Define colors (optional, but helps distinguish lines)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(baseline_actions)))\n",
    "\n",
    "for ax, (column, title) in zip(axes, metrics_to_plot):\n",
    "    for i, (action_val, group) in enumerate(grouped_data):\n",
    "        ax.plot(group['step'], group[column], marker='o', linestyle='-', label=f'Action={action_val}', color=colors[i], markersize=4, alpha=0.8)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Step in Episode')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--', linewidth=0.7)\n",
    "    if column == 'action': # Keep legend only on the first plot for clarity\n",
    "         ax.legend(title=\"Constant Action\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    #else:\n",
    "    #     ax.legend().set_visible(False)\n",
    "\n",
    "\n",
    "# Remove legend from other plots if created automatically and adjust layout\n",
    "# for ax in axes[1:]:\n",
    "#      if ax.get_legend():\n",
    "#           ax.get_legend().remove()\n",
    "\n",
    "fig.suptitle('Step-wise Performance during First Episode for Constant Actions')\n",
    "fig.tight_layout(rect=[0, 0.03, 0.85, 0.95]) # Adjust layout to make space for legend outside\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Display the collected step-wise data\n",
    "print(\"\\nStep-wise data for the first episode of each action:\")\n",
    "display(step_wise_df)eline Performance')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd962bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217580b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
