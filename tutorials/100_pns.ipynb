{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from typing import Tuple\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def glorot_init(key, shape, dtype=jnp.float32):\n",
    "    fan_in, fan_out = shape[0], shape[1]\n",
    "    limit = jnp.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return jax.random.uniform(key, shape, dtype, -limit, limit)\n",
    "\n",
    "# -----------------------------\n",
    "# Simulated data\n",
    "# -----------------------------\n",
    "def simulate_data(\n",
    "    num_individuals: int,\n",
    "    num_markers: int,\n",
    "    key: jax.random.PRNGKey\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "    genotypes = jax.random.randint(\n",
    "        subkey1,\n",
    "        shape=(num_individuals, num_markers * 2),\n",
    "        minval=0,\n",
    "        maxval=2\n",
    "    ).astype(jnp.float32)\n",
    "    phenotypes = jax.random.normal(subkey2, shape=(num_individuals,)).astype(jnp.float32)\n",
    "    return genotypes, phenotypes\n",
    "\n",
    "# -----------------------------\n",
    "# Encoder (creates params outside scan)\n",
    "# -----------------------------\n",
    "class Encoder(nn.Module):\n",
    "    d_model: int\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    dropout_rate: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool):\n",
    "        # initial projection to model dim\n",
    "        x = nn.Dense(self.d_model)(x)\n",
    "        for _ in range(self.num_layers):\n",
    "            x_norm = nn.LayerNorm()(x)\n",
    "            # Use SelfAttention which creates params here (but this call is outside scan)\n",
    "            x_attn = nn.SelfAttention(num_heads=self.num_heads, qkv_features=self.d_model)(x_norm)\n",
    "            x = x + nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x_attn)\n",
    "\n",
    "            x_norm = nn.LayerNorm()(x)\n",
    "            x_ffn = nn.Dense(self.d_model * 4)(x_norm)\n",
    "            x_ffn = nn.relu(x_ffn)\n",
    "            x_ffn = nn.Dense(self.d_model)(x_ffn)\n",
    "            x = x + nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x_ffn)\n",
    "\n",
    "        return nn.LayerNorm()(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Pointer Network (no param creation in scan)\n",
    "# -----------------------------\n",
    "class PointerNetwork(nn.Module):\n",
    "    d_model: int\n",
    "    num_heads: int\n",
    "    num_encoder_layers: int\n",
    "\n",
    "    def setup(self):\n",
    "        # Encoder is a module; calling encoder will create params during init/apply outside scan.\n",
    "        self.encoder = Encoder(d_model=self.d_model, num_heads=self.num_heads, num_layers=self.num_encoder_layers)\n",
    "\n",
    "        # Create projection matrices as explicit params in setup so they are available before scan.\n",
    "        # Shapes: Wk: [d_model, d_model], Wq: [d_model, d_model]\n",
    "        # We'll initialize them here with zeros placeholders; actual initial values are provided via self.param()\n",
    "        # Use lambda initializers that accept rng.\n",
    "        self.Wk = self.param(\"Wk\", lambda k, s=(self.d_model, self.d_model): glorot_init(k, s))\n",
    "        self.Wq = self.param(\"Wq\", lambda k, s=(self.d_model, self.d_model): glorot_init(k, s))\n",
    "\n",
    "        # Optionally a bias for query or keys (not necessary but kept for parity)\n",
    "        self.bk = self.param(\"bk\", lambda k, s=(self.d_model,): jnp.zeros(s))\n",
    "        self.bq = self.param(\"bq\", lambda k, s=(self.d_model,): jnp.zeros(s))\n",
    "\n",
    "    def __call__(self, genotypes: jnp.ndarray, phenotypes: jnp.ndarray, train: bool):\n",
    "        # Combine genotype + phenotype\n",
    "        phenotypes_reshaped = jnp.expand_dims(phenotypes, axis=-1)\n",
    "        inputs = jnp.concatenate([genotypes, phenotypes_reshaped], axis=-1)  # [N, feat]\n",
    "\n",
    "        # Run encoder (this creates/uses encoder params). This happens outside scan.\n",
    "        encoder_outputs = self.encoder(inputs, train)  # shape [N, d_model]\n",
    "\n",
    "        # Precompute normalized keys & key projections outside scan\n",
    "        # We'll use a simple LayerNorm over encoder outputs (the layernorm in encoder already applied; but we can reuse)\n",
    "        # Use the stored Wk to compute key projections once:\n",
    "        # encoder_outputs: [N, d_model], Wk: [d_model, d_model]\n",
    "        key_proj = jnp.dot(encoder_outputs, self.Wk) + self.bk  # [N, d_model]\n",
    "\n",
    "        # We'll use Wq for projecting the last embedding inside scan; Wq is a DeviceArray (static parameter).\n",
    "        # No Flax param creation happens inside scan now.\n",
    "\n",
    "        num_individuals = encoder_outputs.shape[0]\n",
    "\n",
    "        # initial selection: pick index 0 (deterministic demo)\n",
    "        initial_idx = jnp.array(0, dtype=jnp.int32)\n",
    "        # selected array: -1 for empty slots; we will fill positions 0..N-1 with selected indices\n",
    "        selected_array = -jnp.ones((num_individuals,), dtype=jnp.int32).at[0].set(initial_idx)\n",
    "\n",
    "        # boolean mask indicating which indices are already selected\n",
    "        mask = jnp.zeros((num_individuals,), dtype=jnp.bool_).at[0].set(True)\n",
    "\n",
    "        # carry: (last_idx, selected_array, mask, step)\n",
    "        initial_carry = (initial_idx, selected_array, mask, jnp.array(1, dtype=jnp.int32))\n",
    "\n",
    "        def step_fn(carry, _):\n",
    "            last_idx, selected_array, mask, step = carry\n",
    "\n",
    "            # get last embedding (safe traced indexing via jnp.take)\n",
    "            last_emb = jnp.take(encoder_outputs, last_idx, axis=0)  # [d_model]\n",
    "\n",
    "            # Project query using pre-created Wq (no param creation here)\n",
    "            q_proj = jnp.dot(last_emb, self.Wq) + self.bq  # [d_model]\n",
    "\n",
    "            # compute scores: dot(q_proj, key_proj[i]) for each i\n",
    "            # key_proj: [N, d_model], q_proj: [d_model] -> scores: [N]\n",
    "            scores = jnp.einsum(\"d,nd->n\", q_proj, key_proj) / jnp.sqrt(self.d_model)\n",
    "\n",
    "            # mask already selected\n",
    "            scores = jnp.where(mask, -1e9, scores)\n",
    "\n",
    "            # greedy pick\n",
    "            next_idx = jnp.argmax(scores).astype(jnp.int32)\n",
    "\n",
    "            # write into selected array at position `step`\n",
    "            selected_array = selected_array.at[step].set(next_idx)\n",
    "\n",
    "            # update mask\n",
    "            mask = mask.at[next_idx].set(True)\n",
    "\n",
    "            next_step = step + 1\n",
    "            new_carry = (next_idx, selected_array, mask, next_step)\n",
    "            return new_carry, None\n",
    "\n",
    "        # run scan for remaining selections (we already set initial index)\n",
    "        num_steps = num_individuals - 1\n",
    "        final_carry, _ = jax.lax.scan(step_fn, initial_carry, None, length=num_steps)\n",
    "\n",
    "        return final_carry[1]  # selected_array\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    num_individuals = 12   # small demo\n",
    "    num_markers = 8\n",
    "    genotypes, phenotypes = simulate_data(num_individuals, num_markers, key)\n",
    "\n",
    "    key, model_key, dropout_key = jax.random.split(key, 3)\n",
    "    d_model = 64\n",
    "    num_heads = 4\n",
    "    num_encoder_layers = 2\n",
    "\n",
    "    model = PointerNetwork(d_model=d_model, num_heads=num_heads, num_encoder_layers=num_encoder_layers)\n",
    "\n",
    "    print(\"Initializing model parameters...\")\n",
    "    variables = model.init({\"params\": model_key}, genotypes, phenotypes, train=False)\n",
    "    params = variables[\"params\"]\n",
    "    print(\"Initialization complete.\\n\")\n",
    "\n",
    "    print(\"Running forward pass...\")\n",
    "    # forward; dropout not used here so no rngs necessary\n",
    "    selected_sequence = model.apply({\"params\": params}, genotypes, phenotypes, train=False)\n",
    "    print(\"Forward pass complete.\\n\")\n",
    "\n",
    "    print(\"Selected sequence of individuals (indices):\")\n",
    "    print(selected_sequence)\n",
    "\n",
    "    seq_list = list(map(int, jnp.array(selected_sequence).tolist()))\n",
    "    print(\"\\nIs the sequence a permutation (no repeated indices)?\", len(set(seq_list)) == len(seq_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32be774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Input Data Shape ---\n",
      "(4, 10, 50)\n",
      "\n",
      "--- Output Log Probs Shape ---\n",
      "(4, 10, 128)\n",
      "\n",
      "--- Sample Input (first 5 markers of first individual in batch) ---\n",
      "[0 0 0 1 0]\n",
      "\n",
      "--- Predicted Sequence (for first sample in batch) ---\n",
      "[65 65 65 65 65 65 65 65 65 65]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "from typing import Sequence\n",
    "\n",
    "# --- 1. Simulate Input Data ---\n",
    "def simulate_data(num_individuals, num_markers, batch_size):\n",
    "    \"\"\"\n",
    "    Simulates a batch of 0/1 coded vectors.\n",
    "\n",
    "    Args:\n",
    "        num_individuals: The number of individuals in the input sequence.\n",
    "        num_markers: The feature dimension for each individual.\n",
    "        batch_size: The number of samples in the batch.\n",
    "\n",
    "    Returns:\n",
    "        A jnp.ndarray of shape (batch_size, num_individuals, num_markers).\n",
    "    \"\"\"\n",
    "    return jnp.array(np.random.randint(2, size=(batch_size, num_individuals, num_markers)))\n",
    "\n",
    "# --- 2. Pointer Network with Transformer Encoder ---\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"A simple transformer encoder.\"\"\"\n",
    "    num_heads: int\n",
    "    emb_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask=None):\n",
    "        # The nn.MultiHeadDotProductAttention layer is a standard implementation\n",
    "        # of multi-head attention, a key component of the transformer architecture.\n",
    "        x = nn.MultiHeadDotProductAttention(num_heads=self.num_heads, qkv_features=self.emb_dim)(x, mask=mask)\n",
    "\n",
    "        # We apply layer normalization before the feedforward network, which is a common\n",
    "        # practice in transformer architectures (pre-LN).\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        # The feedforward network consists of two dense layers with a GELU activation in between.\n",
    "        # This is a standard component of the transformer architecture.\n",
    "        y = nn.Dense(features=self.emb_dim * 4)(x)\n",
    "        y = nn.gelu(y)\n",
    "        y = nn.Dense(features=self.emb_dim)(y)\n",
    "\n",
    "        # A residual connection is used to add the output of the feedforward network to the input.\n",
    "        return x + y\n",
    "\n",
    "class PointerNetwork(nn.Module):\n",
    "    \"\"\"A Pointer Network with a Transformer Encoder.\"\"\"\n",
    "    num_heads: int\n",
    "    emb_dim: int\n",
    "    num_layers: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, deterministic=True):\n",
    "        # --- Encoder ---\n",
    "        # The input is first passed through a dense layer to project it to the embedding dimension.\n",
    "        x = nn.Dense(features=self.emb_dim, name=\"input_embedding\")(x)\n",
    "\n",
    "        # The encoded input is then passed through a series of transformer encoder layers.\n",
    "        for _ in range(self.num_layers):\n",
    "            x = Encoder(num_heads=self.num_heads, emb_dim=self.emb_dim)(x)\n",
    "\n",
    "        # --- Decoder ---\n",
    "        # The decoder uses a similar architecture to the encoder, with a final dense layer\n",
    "        # to produce the output logits.\n",
    "        logits = nn.Dense(features=x.shape[-1], name=\"output_logits\")(x)\n",
    "\n",
    "        # The output logits are then passed through a softmax function to produce a probability\n",
    "        # distribution over the input individuals.\n",
    "        return nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "\n",
    "# --- 3. Sample Run ---\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "NUM_INDIVIDUALS = 10\n",
    "NUM_MARKERS = 50\n",
    "EMB_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 3\n",
    "BATCH_SIZE = 4\n",
    "KEY = jax.random.PRNGKey(0)\n",
    "\n",
    "# --- Initialize Model ---\n",
    "model = PointerNetwork(num_heads=NUM_HEADS, emb_dim=EMB_DIM, num_layers=NUM_LAYERS)\n",
    "params = model.init(KEY, jnp.ones((BATCH_SIZE, NUM_INDIVIDUALS, NUM_MARKERS)))[\"params\"]\n",
    "\n",
    "# --- Simulate Data and Run Model ---\n",
    "input_data = simulate_data(NUM_INDIVIDUALS, NUM_MARKERS, BATCH_SIZE)\n",
    "output_log_probs = model.apply({\"params\": params}, input_data)\n",
    "\n",
    "# --- Get Predicted Sequence ---\n",
    "predicted_sequence = jnp.argmax(output_log_probs, axis=-1)\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"--- Input Data Shape ---\")\n",
    "print(input_data.shape)\n",
    "print(\"\\n--- Output Log Probs Shape ---\")\n",
    "print(output_log_probs.shape)\n",
    "print(\"\\n--- Sample Input (first 5 markers of first individual in batch) ---\")\n",
    "print(input_data[0, 0, :5])\n",
    "print(\"\\n--- Predicted Sequence (for first sample in batch) ---\")\n",
    "print(predicted_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4f4553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Input Data Shape ---\n",
      "(4, 10, 50)\n",
      "\n",
      "--- Output Log Probs Shape (Corrected) ---\n",
      "(4, 10, 10)\n",
      "\n",
      "--- Sample Input (first 5 markers of first individual in batch) ---\n",
      "[1 1 0 0 0]\n",
      "\n",
      "--- Predicted Sequence (for first sample in batch) ---\n",
      "[0 0 7 4 5 4 0 4 4 3]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "from typing import Sequence\n",
    "\n",
    "# --- 1. Simulate Input Data ---\n",
    "def simulate_data(num_individuals, num_markers, batch_size):\n",
    "    \"\"\"\n",
    "    Simulates a batch of 0/1 coded vectors.\n",
    "\n",
    "    Args:\n",
    "        num_individuals: The number of individuals in the input sequence.\n",
    "        num_markers: The feature dimension for each individual.\n",
    "        batch_size: The number of samples in the batch.\n",
    "\n",
    "    Returns:\n",
    "        A jnp.ndarray of shape (batch_size, num_individuals, num_markers).\n",
    "    \"\"\"\n",
    "    return jnp.array(np.random.randint(2, size=(batch_size, num_individuals, num_markers)))\n",
    "\n",
    "# --- 2. Pointer Network with Transformer Encoder ---\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"A simple transformer encoder.\"\"\"\n",
    "    num_heads: int\n",
    "    emb_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask=None):\n",
    "        x = nn.MultiHeadDotProductAttention(num_heads=self.num_heads, qkv_features=self.emb_dim)(x, mask=mask)\n",
    "        x = nn.LayerNorm()(x)\n",
    "        y = nn.Dense(features=self.emb_dim * 4)(x)\n",
    "        y = nn.gelu(y)\n",
    "        y = nn.Dense(features=self.emb_dim)(y)\n",
    "        return x + y\n",
    "\n",
    "class PointerNetwork(nn.Module):\n",
    "    \"\"\"A Pointer Network with a Transformer Encoder.\"\"\"\n",
    "    num_heads: int\n",
    "    emb_dim: int\n",
    "    num_layers: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, deterministic=True):\n",
    "        # --- Encoder ---\n",
    "        x = nn.Dense(features=self.emb_dim, name=\"input_embedding\")(x)\n",
    "\n",
    "        for _ in range(self.num_layers):\n",
    "            x = Encoder(num_heads=self.num_heads, emb_dim=self.emb_dim)(x)\n",
    "\n",
    "        # --- Decoder (Corrected Pointer Mechanism) ---\n",
    "        # The original code had a Dense layer that outputted a vector of size `emb_dim`.\n",
    "        # The corrected version uses matrix multiplication to create scores for each\n",
    "        # input individual, effectively \"pointing\" to them.\n",
    "        # This results in logits with the correct shape: (batch_size, num_individuals, num_individuals)\n",
    "        logits = jnp.matmul(x, x.transpose((0, 2, 1)))\n",
    "\n",
    "        return nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "\n",
    "# --- 3. Sample Run ---\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "NUM_INDIVIDUALS = 10\n",
    "NUM_MARKERS = 50\n",
    "EMB_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 3\n",
    "BATCH_SIZE = 4\n",
    "KEY = jax.random.PRNGKey(0)\n",
    "\n",
    "# --- Initialize Model ---\n",
    "model = PointerNetwork(num_heads=NUM_HEADS, emb_dim=EMB_DIM, num_layers=NUM_LAYERS)\n",
    "# Pass the correct shape to the init method\n",
    "init_data = jnp.ones((BATCH_SIZE, NUM_INDIVIDUALS, NUM_MARKERS))\n",
    "params = model.init(KEY, init_data)[\"params\"]\n",
    "\n",
    "# --- Simulate Data and Run Model ---\n",
    "input_data = simulate_data(NUM_INDIVIDUALS, NUM_MARKERS, BATCH_SIZE)\n",
    "output_log_probs = model.apply({\"params\": params}, input_data)\n",
    "\n",
    "# --- Get Predicted Sequence ---\n",
    "predicted_sequence = jnp.argmax(output_log_probs, axis=-1)\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"--- Input Data Shape ---\")\n",
    "print(input_data.shape)\n",
    "print(\"\\n--- Output Log Probs Shape (Corrected) ---\")\n",
    "print(output_log_probs.shape)\n",
    "print(\"\\n--- Sample Input (first 5 markers of first individual in batch) ---\")\n",
    "print(input_data[0, 0, :5])\n",
    "print(\"\\n--- Predicted Sequence (for first sample in batch) ---\")\n",
    "print(predicted_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c28ff2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
