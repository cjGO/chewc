{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca988fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e54050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4973ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glect/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import warp: No module named 'warp'\n",
      "Failed to import mujoco_warp: No module named 'warp'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-10-16 21:10:03,299:jax._src.xla_bridge:794: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import chex\n",
    "import numpy as np\n",
    "from flax import struct\n",
    "from functools import partial\n",
    "from typing import Optional, Tuple, Union, Any\n",
    "from gymnax.environments import environment, spaces\n",
    "from brax import envs\n",
    "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
    "import navix as nx\n",
    "\n",
    "\n",
    "class GymnaxWrapper(object):\n",
    "    \"\"\"Base class for Gymnax wrappers.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    # provide proxy access to regular attributes of wrapped object\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "\n",
    "class FlattenObservationWrapper(GymnaxWrapper):\n",
    "    \"\"\"Flatten the observations of the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env: environment.Environment):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def observation_space(self, params) -> spaces.Box:\n",
    "        assert isinstance(\n",
    "            self._env.observation_space(params), spaces.Box\n",
    "        ), \"Only Box spaces are supported for now.\"\n",
    "        return spaces.Box(\n",
    "            low=self._env.observation_space(params).low,\n",
    "            high=self._env.observation_space(params).high,\n",
    "            shape=(np.prod(self._env.observation_space(params).shape),),\n",
    "            dtype=self._env.observation_space(params).dtype,\n",
    "        )\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def reset(\n",
    "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
    "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        obs = jnp.reshape(obs, (-1,))\n",
    "        return obs, state\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def step(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: environment.EnvState,\n",
    "        action: Union[int, float],\n",
    "        params: Optional[environment.EnvParams] = None,\n",
    "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
    "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
    "        obs = jnp.reshape(obs, (-1,))\n",
    "        return obs, state, reward, done, info\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class LogEnvState:\n",
    "    env_state: environment.EnvState\n",
    "    episode_returns: float\n",
    "    episode_lengths: int\n",
    "    returned_episode_returns: float\n",
    "    returned_episode_lengths: int\n",
    "    timestep: int\n",
    "\n",
    "\n",
    "class LogWrapper(GymnaxWrapper):\n",
    "    \"\"\"Log the episode returns and lengths.\"\"\"\n",
    "\n",
    "    def __init__(self, env: environment.Environment):\n",
    "        super().__init__(env)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def reset(\n",
    "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
    "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
    "        obs, env_state = self._env.reset(key, params)\n",
    "        state = LogEnvState(env_state, 0, 0, 0, 0, 0)\n",
    "        return obs, state\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def step(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: environment.EnvState,\n",
    "        action: Union[int, float],\n",
    "        params: Optional[environment.EnvParams] = None,\n",
    "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "        new_episode_return = state.episode_returns + reward\n",
    "        new_episode_length = state.episode_lengths + 1\n",
    "        state = LogEnvState(\n",
    "            env_state=env_state,\n",
    "            episode_returns=new_episode_return * (1 - done),\n",
    "            episode_lengths=new_episode_length * (1 - done),\n",
    "            returned_episode_returns=state.returned_episode_returns * (1 - done)\n",
    "            + new_episode_return * done,\n",
    "            returned_episode_lengths=state.returned_episode_lengths * (1 - done)\n",
    "            + new_episode_length * done,\n",
    "            timestep=state.timestep + 1,\n",
    "        )\n",
    "        info[\"returned_episode_returns\"] = state.returned_episode_returns\n",
    "        info[\"returned_episode_lengths\"] = state.returned_episode_lengths\n",
    "        info[\"timestep\"] = state.timestep\n",
    "        info[\"returned_episode\"] = done\n",
    "        return obs, state, reward, done, info\n",
    "\n",
    "\n",
    "class BraxGymnaxWrapper:\n",
    "    def __init__(self, env_name, backend=\"positional\"):\n",
    "        env = envs.get_environment(env_name=env_name, backend=backend)\n",
    "        env = EpisodeWrapper(env, episode_length=1000, action_repeat=1)\n",
    "        env = AutoResetWrapper(env)\n",
    "        self._env = env\n",
    "        self.action_size = env.action_size\n",
    "        self.observation_size = (env.observation_size,)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        state = self._env.reset(key)\n",
    "        return state.obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        next_state = self._env.step(state, action)\n",
    "        return next_state.obs, next_state, next_state.reward, next_state.done > 0.5, {}\n",
    "\n",
    "    def observation_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=-jnp.inf,\n",
    "            high=jnp.inf,\n",
    "            shape=(self._env.observation_size,),\n",
    "        )\n",
    "\n",
    "    def action_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(self._env.action_size,),\n",
    "        )\n",
    "\n",
    "class NavixGymnaxWrapper:\n",
    "    def __init__(self, env_name):\n",
    "        self._env = nx.make(env_name)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        timestep = self._env.reset(key)\n",
    "        return timestep.observation, timestep\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        timestep = self._env.step(state, action)\n",
    "        return timestep.observation, timestep, timestep.reward, timestep.is_done(), {}\n",
    "\n",
    "    def observation_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=self._env.observation_space.minimum,\n",
    "            high=self._env.observation_space.maximum,\n",
    "            shape=(np.prod(self._env.observation_space.shape),),\n",
    "            dtype=self._env.observation_space.dtype,\n",
    "        )\n",
    "\n",
    "    def action_space(self, params):\n",
    "        return spaces.Discrete(\n",
    "            num_categories=self._env.action_space.maximum.item() + 1,\n",
    "        )\n",
    "\n",
    "\n",
    "class ClipAction(GymnaxWrapper):\n",
    "    def __init__(self, env, low=-1.0, high=1.0):\n",
    "        super().__init__(env)\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        \"\"\"TODO: In theory the below line should be the way to do this.\"\"\"\n",
    "        # action = jnp.clip(action, self.env.action_space.low, self.env.action_space.high)\n",
    "        action = jnp.clip(action, self.low, self.high)\n",
    "        return self._env.step(key, state, action, params)\n",
    "\n",
    "\n",
    "class TransformObservation(GymnaxWrapper):\n",
    "    def __init__(self, env, transform_obs):\n",
    "        super().__init__(env)\n",
    "        self.transform_obs = transform_obs\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        return self.transform_obs(obs), state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
    "        return self.transform_obs(obs), state, reward, done, info\n",
    "\n",
    "\n",
    "class TransformReward(GymnaxWrapper):\n",
    "    def __init__(self, env, transform_reward):\n",
    "        super().__init__(env)\n",
    "        self.transform_reward = transform_reward\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
    "        return obs, state, self.transform_reward(reward), done, info\n",
    "\n",
    "\n",
    "class VecEnv(GymnaxWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.reset = jax.vmap(self._env.reset, in_axes=(0, None))\n",
    "        self.step = jax.vmap(self._env.step, in_axes=(0, 0, 0, None))\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizeVecObsEnvState:\n",
    "    mean: jnp.ndarray\n",
    "    var: jnp.ndarray\n",
    "    count: float\n",
    "    env_state: environment.EnvState\n",
    "\n",
    "\n",
    "class NormalizeVecObservation(GymnaxWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=jnp.zeros_like(obs),\n",
    "            var=jnp.ones_like(obs),\n",
    "            count=1e-4,\n",
    "            env_state=state,\n",
    "        )\n",
    "        batch_mean = jnp.mean(obs, axis=0)\n",
    "        batch_var = jnp.var(obs, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            env_state=state.env_state,\n",
    "        )\n",
    "\n",
    "        return (obs - state.mean) / jnp.sqrt(state.var + 1e-8), state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "\n",
    "        batch_mean = jnp.mean(obs, axis=0)\n",
    "        batch_var = jnp.var(obs, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        return (\n",
    "            (obs - state.mean) / jnp.sqrt(state.var + 1e-8),\n",
    "            state,\n",
    "            reward,\n",
    "            done,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizeVecRewEnvState:\n",
    "    mean: jnp.ndarray\n",
    "    var: jnp.ndarray\n",
    "    count: float\n",
    "    return_val: float\n",
    "    env_state: environment.EnvState\n",
    "\n",
    "\n",
    "class NormalizeVecReward(GymnaxWrapper):\n",
    "    def __init__(self, env, gamma):\n",
    "        super().__init__(env)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        batch_count = obs.shape[0]\n",
    "        state = NormalizeVecRewEnvState(\n",
    "            mean=0.0,\n",
    "            var=1.0,\n",
    "            count=1e-4,\n",
    "            return_val=jnp.zeros((batch_count,)),\n",
    "            env_state=state,\n",
    "        )\n",
    "        return obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "        return_val = state.return_val * self.gamma * (1 - done) + reward\n",
    "\n",
    "        batch_mean = jnp.mean(return_val, axis=0)\n",
    "        batch_var = jnp.var(return_val, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecRewEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            return_val=return_val,\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        return obs, state, reward / jnp.sqrt(state.var + 1e-8), done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e4ba7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
