{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict\n",
    "\n",
    "> Common operations around the core datastructures for running a sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from flax.struct import dataclass as flax_dataclass\n",
    "import jax.numpy as jnp\n",
    "from typing import Optional\n",
    "from chewc.population import Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\"\"\"\n",
    "GBLUP (Genomic Best Linear Unbiased Prediction) implementation for chewc library.\n",
    "\n",
    "This module provides functions for genomic prediction using the GBLUP methodology,\n",
    "which is a standard approach in genomic selection and animal breeding.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Optional, Dict\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.numpy.linalg import solve, inv, pinv\n",
    "from flax.struct import dataclass as flax_dataclass\n",
    "\n",
    "from chewc.population import Population\n",
    "\n",
    "@flax_dataclass(frozen=True)\n",
    "class PredictionResults:\n",
    "    \"\"\"\n",
    "    A container for the results of a genomic prediction.\n",
    "    \"\"\"\n",
    "    ids: jnp.ndarray\n",
    "    ebv: jnp.ndarray\n",
    "    pev: Optional[jnp.ndarray] = None\n",
    "    reliability: Optional[jnp.ndarray] = None\n",
    "    fixed_effects: Optional[jnp.ndarray] = None\n",
    "    h2_used: Optional[float] = None\n",
    "    var_components: Optional[Dict] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from functools import partial\n",
    "from typing import Optional, Dict, Tuple\n",
    "from jax.scipy.sparse.linalg import cg\n",
    "from jax import lax\n",
    "\n",
    "# ==================================================\n",
    "# --- Core Matrix Calculation Functions ---\n",
    "# ==================================================\n",
    "@partial(jax.jit, static_argnames=('n_ind',))\n",
    "def _jit_calc_a_inverse(mother_iids: jnp.ndarray, father_iids: jnp.ndarray, n_ind: int) -> jnp.ndarray:\n",
    "    initial_A_inv = jnp.zeros((n_ind, n_ind))\n",
    "    def loop_body(i, A_inv):\n",
    "        sire_iid, dam_iid = father_iids[i], mother_iids[i]\n",
    "        case_index = (sire_iid != -1) + 2 * (dam_iid != -1)\n",
    "        def case_0(mat): return mat.at[i, i].add(1.0)\n",
    "        def case_1(mat): return mat.at[i, i].add(4/3.).at[sire_iid, sire_iid].add(1/3.).at[i, sire_iid].add(-2/3.).at[sire_iid, i].add(-2/3.)\n",
    "        def case_2(mat): return mat.at[i, i].add(4/3.).at[dam_iid, dam_iid].add(1/3.).at[i, dam_iid].add(-2/3.).at[dam_iid, i].add(-2/3.)\n",
    "        def case_3(mat): return (mat.at[i, i].add(2.0).at[sire_iid, sire_iid].add(0.5).at[dam_iid, dam_iid].add(0.5)\n",
    "                                 .at[sire_iid, dam_iid].add(0.5).at[dam_iid, sire_iid].add(0.5).at[i, sire_iid].add(-1.0)\n",
    "                                 .at[sire_iid, i].add(-1.0).at[i, dam_iid].add(-1.0).at[dam_iid, i].add(-1.0))\n",
    "        return lax.switch(case_index, [case_0, case_1, case_2, case_3], A_inv)\n",
    "    return lax.fori_loop(0, n_ind, loop_body, initial_A_inv)\n",
    "\n",
    "def calc_a_inverse_matrix_pedigree_jax(pop: Population) -> jnp.ndarray:\n",
    "    n_ind = pop.nInd\n",
    "    id_to_iid_map = jnp.full(pop.id.max() + 2, -1, dtype=jnp.int32).at[pop.id].set(pop.iid)\n",
    "    mother_iids = jnp.where(pop.mother < 0, -1, id_to_iid_map[pop.mother.clip(min=0)])\n",
    "    father_iids = jnp.where(pop.father < 0, -1, id_to_iid_map[pop.father.clip(min=0)])\n",
    "    return _jit_calc_a_inverse(mother_iids, father_iids, n_ind)\n",
    "\n",
    "@jax.jit\n",
    "def calc_g_matrix(geno_dosage: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the genomic relationship matrix (G).\n",
    "    --- FIX ---\n",
    "    The original epsilon was too small to prevent the G matrix from becoming\n",
    "    ill-conditioned in later generations. Increasing it slightly adds a larger\n",
    "    \"ridge\" to the diagonal, making the matrix more stable and invertible.\n",
    "    We also blend G with the identity matrix, which is a common technique to\n",
    "    improve the conditioning of G.\n",
    "    \"\"\"\n",
    "    n_ind, n_markers = geno_dosage.shape\n",
    "    p = jnp.mean(geno_dosage, axis=0) / 2.0\n",
    "    P = 2 * p\n",
    "    M = geno_dosage - P\n",
    "    denominator = 2 * jnp.sum(p * (1 - p))\n",
    "    denominator = jnp.where(denominator == 0, 1e-8, denominator)\n",
    "    G_raw = (M @ M.T) / denominator\n",
    "\n",
    "    # --- FIX ---\n",
    "    # Blend G with the identity matrix to improve conditioning.\n",
    "    # 0.99 is a common blending factor.\n",
    "    G = 0.99 * G_raw + 0.01 * jnp.identity(n_ind)\n",
    "    # Add a slightly larger epsilon for numerical stability.\n",
    "    epsilon = 1e-3\n",
    "    return G + jnp.identity(n_ind) * epsilon\n",
    "\n",
    "@partial(jax.jit, static_argnames='n_ind')\n",
    "def _mme_solver_cg(pheno: jnp.ndarray, train_mask: jnp.ndarray, K_inv: jnp.ndarray, h2: float, n_ind: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Mixed model equation (MME) solver using the conjugate gradient algorithm.\n",
    "    --- FIX ---\n",
    "    The preconditioner is improved by adding a small offset to the diagonal elements.\n",
    "    This prevents division by very small numbers if any diagonal elements of K_inv\n",
    "    are close to zero, which can happen with ill-conditioned matrices.\n",
    "    \"\"\"\n",
    "    alpha = (1.0 - h2) / h2\n",
    "    y = jnp.nan_to_num(pheno.flatten())\n",
    "    train_mask_float = train_mask.astype(jnp.float32)\n",
    "    n_train = jnp.sum(train_mask_float)\n",
    "\n",
    "    def lhs_matvec(solution_vector):\n",
    "        beta, u = solution_vector[0], solution_vector[1:]\n",
    "        row1 = n_train * beta + jnp.sum(u * train_mask_float)\n",
    "        row2 = train_mask_float * beta + u * train_mask_float + (K_inv @ u) * alpha\n",
    "        return jnp.concatenate([jnp.array([row1]), row2])\n",
    "\n",
    "    rhs = jnp.concatenate([\n",
    "        jnp.array([jnp.sum(y * train_mask_float)]),\n",
    "        y * train_mask_float\n",
    "    ])\n",
    "\n",
    "    # --- FIX ---\n",
    "    # A more robust diagonal preconditioner. Adding a small constant (1e-6)\n",
    "    # ensures that we don't divide by zero, even if some diagonal\n",
    "    # elements are very small.\n",
    "    M_diag = jnp.concatenate([\n",
    "        jnp.array([n_train]),\n",
    "        train_mask_float + jnp.diag(K_inv) * alpha + 1e-6\n",
    "    ])\n",
    "    M_diag = jnp.maximum(M_diag, 1e-6)\n",
    "    # Increase the tolerance and max iterations for the CG solver for more stability\n",
    "    solutions, _ = cg(lhs_matvec, rhs, M=lambda x: x / M_diag, tol=1e-5, maxiter=2*n_ind)\n",
    "    return solutions[0:1], solutions[1:]\n",
    "\n",
    "# ==================================================\n",
    "# --- Prediction Functions (ABLUP and GBLUP) ---\n",
    "# ==================================================\n",
    "def mme_predict_ablup(pop: Population, h2: float, trait_idx: int = 0) -> PredictionResults:\n",
    "    pheno = pop.pheno[:, trait_idx:trait_idx+1]\n",
    "    n_ind = pop.nInd\n",
    "    train_mask = ~jnp.isnan(pheno.flatten())\n",
    "    if jnp.sum(train_mask) == 0: raise ValueError(\"No individuals with phenotypes.\")\n",
    "    K_inv = calc_a_inverse_matrix_pedigree_jax(pop)\n",
    "    # --- FIX ---\n",
    "    # Call the unified MME solver with the inverse of the pedigree relationship matrix (A_inv).\n",
    "    fixed_effects, all_ebv = _mme_solver_cg(pheno, train_mask, K_inv, h2, n_ind)\n",
    "    return PredictionResults(ids=pop.id, ebv=all_ebv.reshape(-1, 1), fixed_effects=fixed_effects, h2_used=h2)\n",
    "\n",
    "def mme_predict_gblup(pop: Population, h2: float, trait_idx: int = 0) -> PredictionResults:\n",
    "    \"\"\"\n",
    "    Predicts Estimated Breeding Values (EBVs) using Genomic Best Linear Unbiased Prediction (GBLUP).\n",
    "    This version includes jax.debug.print statements to diagnose NaN issues.\n",
    "    \"\"\"\n",
    "    pheno = pop.pheno[:, trait_idx:trait_idx+1]\n",
    "    n_ind = pop.nInd\n",
    "    train_mask = ~jnp.isnan(pheno.flatten())\n",
    "\n",
    "    if jnp.sum(train_mask) == 0:\n",
    "        raise ValueError(\"No individuals with phenotypes.\")\n",
    "\n",
    "    # --- JAX DEBUG STATEMENTS ---\n",
    "    jax.debug.print(\"--- GBLUP Debug Start ---\")\n",
    "    jax.debug.print(\"h2: {h2}\", h2=h2)\n",
    "    jax.debug.print(\"Number of individuals: {n}\", n=n_ind)\n",
    "    jax.debug.print(\"Number of phenotyped individuals: {p}\", p=jnp.sum(train_mask))\n",
    "    jax.debug.print(\"Pheno shape: {s}, contains NaNs: {n}\", s=pheno.shape, n=jnp.any(jnp.isnan(pheno)))\n",
    "    # --- END DEBUG ---\n",
    "\n",
    "    G = calc_g_matrix(pop.dosage)\n",
    "\n",
    "    # --- JAX DEBUG STATEMENTS ---\n",
    "    jax.debug.print(\"G matrix shape: {s}\", s=G.shape)\n",
    "    jax.debug.print(\"G matrix contains NaNs: {n}\", n=jnp.any(jnp.isnan(G)))\n",
    "    jax.debug.print(\"G matrix diagonal min/max: {min}/{max}\", min=jnp.min(jnp.diag(G)), max=jnp.max(jnp.diag(G)))\n",
    "    # Check the condition number to assess if G is ill-conditioned (close to singular)\n",
    "    cond_g = jnp.linalg.cond(G)\n",
    "    jax.debug.print(\"G matrix condition number: {c}\", c=cond_g)\n",
    "    # --- END DEBUG ---\n",
    "\n",
    "    # Invert the G matrix. A large condition number might cause NaNs here.\n",
    "    G_inv = jnp.linalg.inv(G)\n",
    "\n",
    "    # --- JAX DEBUG STATEMENTS ---\n",
    "    jax.debug.print(\"G_inv matrix contains NaNs: {n}\", n=jnp.any(jnp.isnan(G_inv)))\n",
    "    # --- END DEBUG ---\n",
    "\n",
    "    fixed_effects, all_ebv = _mme_solver_cg(pheno, train_mask, G_inv, h2, n_ind)\n",
    "\n",
    "    # --- JAX DEBUG STATEMENTS ---\n",
    "    jax.debug.print(\"Solved fixed_effects contains NaNs: {n}\", n=jnp.any(jnp.isnan(fixed_effects)))\n",
    "    jax.debug.print(\"Solved all_ebv contains NaNs: {n}\", n=jnp.any(jnp.isnan(all_ebv)))\n",
    "    jax.debug.print(\"--- GBLUP Debug End ---\")\n",
    "    # --- END DEBUG ---\n",
    "\n",
    "    return PredictionResults(ids=pop.id, ebv=all_ebv.reshape(-1, 1), fixed_effects=fixed_effects, h2_used=h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
