{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7461920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79154a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75002b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9307b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7055cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-10-15 05:25:43,332:jax._src.xla_bridge:794: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document represents a template for a simple breeding simulation for selection. It is meant to be a guide to developing my complex or flexible breeding designs.\n",
      "--- Running 10-Cycle Phenotypic Selection Experiment ---\n",
      "\n",
      "Step 1: Initializing founder population...\n",
      "Step 2: Initializing trait architecture...\n",
      "  - Founder population size: 100\n",
      "  - Selection: Top 20 individuals\n",
      "  - Offspring per cycle: 200\n",
      "\n",
      "Step 3: Running burn-in cycle to stabilize population size...\n",
      "\n",
      "Step 4: Running remaining 19 selection cycles (JIT compiling...)\n",
      "\n",
      "--- Results ---\n",
      "Generation 00 Mean TBV: 0.816\n",
      "Generation 01 Mean TBV: 0.816 (Gain: +0.000)\n",
      "Generation 02 Mean TBV: 2.206 (Gain: +1.390)\n",
      "Generation 03 Mean TBV: 3.775 (Gain: +1.569)\n",
      "Generation 04 Mean TBV: 5.289 (Gain: +1.514)\n",
      "Generation 05 Mean TBV: 6.590 (Gain: +1.301)\n",
      "Generation 06 Mean TBV: 7.983 (Gain: +1.394)\n",
      "Generation 07 Mean TBV: 9.112 (Gain: +1.129)\n",
      "Generation 08 Mean TBV: 10.077 (Gain: +0.965)\n",
      "Generation 09 Mean TBV: 10.777 (Gain: +0.700)\n",
      "Generation 10 Mean TBV: 11.417 (Gain: +0.640)\n",
      "Generation 11 Mean TBV: 11.958 (Gain: +0.541)\n",
      "Generation 12 Mean TBV: 12.409 (Gain: +0.451)\n",
      "Generation 13 Mean TBV: 12.898 (Gain: +0.490)\n",
      "Generation 14 Mean TBV: 13.360 (Gain: +0.461)\n",
      "Generation 15 Mean TBV: 13.782 (Gain: +0.422)\n",
      "Generation 16 Mean TBV: 14.220 (Gain: +0.438)\n",
      "Generation 17 Mean TBV: 14.535 (Gain: +0.315)\n",
      "Generation 18 Mean TBV: 14.894 (Gain: +0.359)\n",
      "Generation 19 Mean TBV: 15.138 (Gain: +0.244)\n",
      "Generation 20 Mean TBV: 15.476 (Gain: +0.338)\n",
      "\n",
      "--- Total Genetic Gain over 20 generations: +14.660 ---\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from chewc.structs import *\n",
    "from chewc.pheno import *\n",
    "from chewc.select import *\n",
    "from chewc.cross import *\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from functools import partial\n",
    "from typing import Tuple\n",
    "\n",
    "print(\"This document represents a template for a simple breeding simulation for selection. It is meant to be a guide to developing my complex or flexible breeding designs.\")\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"n_select\", \"n_offspring\", \"max_crossovers\"))\n",
    "def selection_step(\n",
    "    carry: BreedingState,\n",
    "    _, # Placeholder for lax.scan's iteration number\n",
    "    trait: Trait,\n",
    "    genetic_map: GeneticMap,\n",
    "    heritabilities: jnp.ndarray,\n",
    "    n_select: int,\n",
    "    n_offspring: int,\n",
    "    max_crossovers: int\n",
    ") -> Tuple[BreedingState, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Executes one full cycle of selection and breeding.\n",
    "    This function is designed to be the body of a lax.scan loop.\n",
    "    \"\"\"\n",
    "    key, pheno_key, mating_key, cross_key = jax.random.split(carry.key, 4)\n",
    "    current_pop = carry.population\n",
    "\n",
    "    # 1. Evaluate the population\n",
    "    phenotypes, tbvs = calculate_phenotypes(\n",
    "        key=pheno_key, population=current_pop, trait=trait, heritability=heritabilities\n",
    "    )\n",
    "    mean_tbv = jnp.mean(tbvs[:, 0]) # Track genetic gain for trait 1\n",
    "\n",
    "    # 2. Select top parents\n",
    "    selected_parents = select_top_k(current_pop, phenotypes[:, 0], k=n_select)\n",
    "\n",
    "    # 3. Generate a random mating plan\n",
    "    pairings = random_mating(mating_key, n_parents=n_select, n_crosses=n_offspring)\n",
    "    mother_indices, father_indices = pairings[:, 0], pairings[:, 1]\n",
    "\n",
    "    # 4. Create the next generation\n",
    "    mothers_geno = selected_parents.geno[mother_indices]\n",
    "    fathers_geno = selected_parents.geno[father_indices]\n",
    "    mothers_ibd = selected_parents.ibd[mother_indices]\n",
    "    fathers_ibd = selected_parents.ibd[father_indices]\n",
    "\n",
    "    # Vectorize the crossing operation over all pairs\n",
    "    vmapped_cross = jax.vmap(\n",
    "        cross_pair, in_axes=(0, 0, 0, 0, 0, None, None)\n",
    "    )\n",
    "    offspring_keys = jax.random.split(cross_key, n_offspring)\n",
    "    offspring_geno, offspring_ibd = vmapped_cross(\n",
    "        offspring_keys, mothers_geno, fathers_geno, mothers_ibd, fathers_ibd, genetic_map, max_crossovers\n",
    "    )\n",
    "\n",
    "    # 5. Form the new population and update the state\n",
    "    new_generation = carry.generation + 1\n",
    "    new_ids = jnp.arange(n_offspring, dtype=jnp.int32) + carry.next_id\n",
    "    new_meta = jnp.stack(\n",
    "        [\n",
    "            new_ids,\n",
    "            selected_parents.meta[mother_indices, 0],\n",
    "            selected_parents.meta[father_indices, 0],\n",
    "            jnp.full((n_offspring,), new_generation, dtype=jnp.int32),\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    new_population = Population(geno=offspring_geno, ibd=offspring_ibd, meta=new_meta)\n",
    "\n",
    "    next_state = BreedingState(\n",
    "        population=new_population,\n",
    "        key=key,\n",
    "        generation=new_generation,\n",
    "        next_id=carry.next_id + n_offspring\n",
    "    )\n",
    "\n",
    "    # The scan function requires a `(carry, output)` return signature.\n",
    "    # `carry` is the state for the next iteration.\n",
    "    # `output` is the data we want to collect at each step.\n",
    "    return next_state, mean_tbv\n",
    "\n",
    "\n",
    "def run_simulation_cycles(\n",
    "    initial_state: BreedingState,\n",
    "    trait: Trait,\n",
    "    genetic_map: GeneticMap,\n",
    "    heritabilities: jnp.ndarray,\n",
    "    n_cycles: int,\n",
    "    n_select: int,\n",
    "    n_offspring: int,\n",
    "    max_crossovers: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the entire multi-cycle simulation using lax.scan for maximum performance.\n",
    "    \"\"\"\n",
    "    # `lax.scan` requires the body function to be a callable that takes\n",
    "    # (carry, x) and returns (new_carry, y). We use a lambda to fix the\n",
    "    # static arguments and data that doesn't change over the loop.\n",
    "    scan_fn = lambda carry, _: selection_step(\n",
    "        carry,\n",
    "        _,\n",
    "        trait=trait,\n",
    "        genetic_map=genetic_map,\n",
    "        heritabilities=heritabilities,\n",
    "        n_select=n_select,\n",
    "        n_offspring=n_offspring,\n",
    "        max_crossovers=max_crossovers\n",
    "    )\n",
    "\n",
    "    # Run the scan. The `None` is a placeholder for the `xs` array,\n",
    "    # as we only care about the number of iterations (`length`).\n",
    "    final_state, tbv_history = lax.scan(\n",
    "        scan_fn, initial_state, None, length=n_cycles\n",
    "    )\n",
    "\n",
    "    return final_state, tbv_history\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Main execution script\n",
    "# ----------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Running 10-Cycle Phenotypic Selection Experiment ---\")\n",
    "\n",
    "    # --- Simulation Parameters ---\n",
    "    N_FOUNDERS = 100\n",
    "    N_SELECT = 20\n",
    "    N_OFFSPRING = 200 # Population size is kept constant after the first cycle\n",
    "    N_CYCLES = 20\n",
    "\n",
    "    N_CHR, N_LOCI = 5, 1000\n",
    "    MAX_CROSSOVERS = 10\n",
    "    SEED = 42\n",
    "\n",
    "    key = jax.random.PRNGKey(SEED)\n",
    "\n",
    "    # --- Setup ---\n",
    "    key, pop_key, trait_key = jax.random.split(key, 3)\n",
    "    print(\"\\nStep 1: Initializing founder population...\")\n",
    "    founder_pop, genetic_map = quick_haplo(\n",
    "        key=pop_key, n_ind=N_FOUNDERS, n_chr=N_CHR, seg_sites=N_LOCI,\n",
    "        inbred=False, chr_length=1.0\n",
    "    )\n",
    "\n",
    "    print(\"Step 2: Initializing trait architecture...\")\n",
    "    trait_architecture = add_trait(\n",
    "        key=trait_key, founder_pop=founder_pop, n_qtl_per_chr=50,\n",
    "        mean=jnp.array([0.0]), var_a=jnp.array([1.0]), var_d=jnp.array([0.0]),\n",
    "        sigma=jnp.array([[10.0]])\n",
    "    )\n",
    "    heritabilities = jnp.array([0.9])\n",
    "\n",
    "    # --- Initial State ---\n",
    "    initial_state = BreedingState(\n",
    "        population=founder_pop,\n",
    "        key=key,\n",
    "        generation=0,\n",
    "        next_id=N_FOUNDERS # Next available individual ID\n",
    "    )\n",
    "    print(f\"  - Founder population size: {N_FOUNDERS}\")\n",
    "    print(f\"  - Selection: Top {N_SELECT} individuals\")\n",
    "    print(f\"  - Offspring per cycle: {N_OFFSPRING}\")\n",
    "\n",
    "    # --- Burn-in Cycle ---\n",
    "    print(\"\\nStep 3: Running burn-in cycle to stabilize population size...\")\n",
    "    \n",
    "    # Calculate initial TBV for generation 0\n",
    "    key, initial_pheno_key = jax.random.split(initial_state.key)\n",
    "    _, initial_tbvs = calculate_phenotypes(\n",
    "        key=initial_pheno_key, population=initial_state.population,\n",
    "        trait=trait_architecture, heritability=heritabilities\n",
    "    )\n",
    "    initial_mean_tbv = jnp.mean(initial_tbvs[:, 0])\n",
    "\n",
    "    # Run the first selection step manually\n",
    "    burn_in_state, first_cycle_tbv = selection_step(\n",
    "        carry=initial_state,\n",
    "        _ = 0, # Placeholder for iteration number\n",
    "        trait=trait_architecture,\n",
    "        genetic_map=genetic_map,\n",
    "        heritabilities=heritabilities,\n",
    "        n_select=N_SELECT,\n",
    "        n_offspring=N_OFFSPRING,\n",
    "        max_crossovers=MAX_CROSSOVERS,\n",
    "    )\n",
    "\n",
    "    # --- Run Simulation ---\n",
    "    print(f\"\\nStep 4: Running remaining {N_CYCLES - 1} selection cycles (JIT compiling...)\\n\")\n",
    "    \n",
    "    # Run the rest of the simulation using lax.scan\n",
    "    final_state, tbv_history = run_simulation_cycles(\n",
    "        initial_state=burn_in_state,\n",
    "        trait=trait_architecture,\n",
    "        genetic_map=genetic_map,\n",
    "        heritabilities=heritabilities,\n",
    "        n_cycles=N_CYCLES - 1,\n",
    "        n_select=N_SELECT,\n",
    "        n_offspring=N_OFFSPRING,\n",
    "        max_crossovers=MAX_CROSSOVERS,\n",
    "    )\n",
    "\n",
    "    # --- Report Results ---\n",
    "    print(\"--- Results ---\")\n",
    "    print(f\"Generation 00 Mean TBV: {initial_mean_tbv:.3f}\")\n",
    "    print(f\"Generation 01 Mean TBV: {first_cycle_tbv:.3f} (Gain: {first_cycle_tbv - initial_mean_tbv:+.3f})\")\n",
    "\n",
    "    # Combine initial TBV, burn-in TBV, and the history from the scan\n",
    "    full_history = jnp.concatenate([jnp.array([initial_mean_tbv, first_cycle_tbv]), tbv_history])\n",
    "    for i, tbv in enumerate(tbv_history, 2):\n",
    "        gain = tbv - full_history[i-1]\n",
    "        print(f\"Generation {i:02d} Mean TBV: {tbv:.3f} (Gain: {gain:+.3f})\")\n",
    "\n",
    "    total_gain = tbv_history[-1] - initial_mean_tbv\n",
    "    print(f\"\\n--- Total Genetic Gain over {N_CYCLES} generations: {total_gain:+.3f} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b948a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fcad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b433a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c575383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a1b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b548e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import chex\n",
    "import numpy as np\n",
    "from flax import struct\n",
    "from functools import partial\n",
    "from typing import Optional, Tuple, Union, Any\n",
    "from gymnax.environments import environment, spaces\n",
    "from brax import envs\n",
    "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
    "import navix as nx\n",
    "\n",
    "\n",
    "class GymnaxWrapper(object):\n",
    "    \"\"\"Base class for Gymnax wrappers.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    # provide proxy access to regular attributes of wrapped object\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "\n",
    "class FlattenObservationWrapper(GymnaxWrapper):\n",
    "    \"\"\"Flatten the observations of the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env: environment.Environment):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def observation_space(self, params) -> spaces.Box:\n",
    "        assert isinstance(\n",
    "            self._env.observation_space(params), spaces.Box\n",
    "        ), \"Only Box spaces are supported for now.\"\n",
    "        return spaces.Box(\n",
    "            low=self._env.observation_space(params).low,\n",
    "            high=self._env.observation_space(params).high,\n",
    "            shape=(np.prod(self._env.observation_space(params).shape),),\n",
    "            dtype=self._env.observation_space(params).dtype,\n",
    "        )\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def reset(\n",
    "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
    "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        obs = jnp.reshape(obs, (-1,))\n",
    "        return obs, state\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def step(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: environment.EnvState,\n",
    "        action: Union[int, float],\n",
    "        params: Optional[environment.EnvParams] = None,\n",
    "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
    "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
    "        obs = jnp.reshape(obs, (-1,))\n",
    "        return obs, state, reward, done, info\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class LogEnvState:\n",
    "    env_state: environment.EnvState\n",
    "    episode_returns: float\n",
    "    episode_lengths: int\n",
    "    returned_episode_returns: float\n",
    "    returned_episode_lengths: int\n",
    "    timestep: int\n",
    "\n",
    "\n",
    "class LogWrapper(GymnaxWrapper):\n",
    "    \"\"\"Log the episode returns and lengths.\"\"\"\n",
    "\n",
    "    def __init__(self, env: environment.Environment):\n",
    "        super().__init__(env)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def reset(\n",
    "        self, key: chex.PRNGKey, params: Optional[environment.EnvParams] = None\n",
    "    ) -> Tuple[chex.Array, environment.EnvState]:\n",
    "        obs, env_state = self._env.reset(key, params)\n",
    "        state = LogEnvState(env_state, 0, 0, 0, 0, 0)\n",
    "        return obs, state\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def step(\n",
    "        self,\n",
    "        key: chex.PRNGKey,\n",
    "        state: environment.EnvState,\n",
    "        action: Union[int, float],\n",
    "        params: Optional[environment.EnvParams] = None,\n",
    "    ) -> Tuple[chex.Array, environment.EnvState, float, bool, dict]:\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "        new_episode_return = state.episode_returns + reward\n",
    "        new_episode_length = state.episode_lengths + 1\n",
    "        state = LogEnvState(\n",
    "            env_state=env_state,\n",
    "            episode_returns=new_episode_return * (1 - done),\n",
    "            episode_lengths=new_episode_length * (1 - done),\n",
    "            returned_episode_returns=state.returned_episode_returns * (1 - done)\n",
    "            + new_episode_return * done,\n",
    "            returned_episode_lengths=state.returned_episode_lengths * (1 - done)\n",
    "            + new_episode_length * done,\n",
    "            timestep=state.timestep + 1,\n",
    "        )\n",
    "        info[\"returned_episode_returns\"] = state.returned_episode_returns\n",
    "        info[\"returned_episode_lengths\"] = state.returned_episode_lengths\n",
    "        info[\"timestep\"] = state.timestep\n",
    "        info[\"returned_episode\"] = done\n",
    "        return obs, state, reward, done, info\n",
    "\n",
    "\n",
    "class BraxGymnaxWrapper:\n",
    "    def __init__(self, env_name, backend=\"positional\"):\n",
    "        env = envs.get_environment(env_name=env_name, backend=backend)\n",
    "        env = EpisodeWrapper(env, episode_length=1000, action_repeat=1)\n",
    "        env = AutoResetWrapper(env)\n",
    "        self._env = env\n",
    "        self.action_size = env.action_size\n",
    "        self.observation_size = (env.observation_size,)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        state = self._env.reset(key)\n",
    "        return state.obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        next_state = self._env.step(state, action)\n",
    "        return next_state.obs, next_state, next_state.reward, next_state.done > 0.5, {}\n",
    "\n",
    "    def observation_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=-jnp.inf,\n",
    "            high=jnp.inf,\n",
    "            shape=(self._env.observation_size,),\n",
    "        )\n",
    "\n",
    "    def action_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(self._env.action_size,),\n",
    "        )\n",
    "\n",
    "class NavixGymnaxWrapper:\n",
    "    def __init__(self, env_name):\n",
    "        self._env = nx.make(env_name)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        timestep = self._env.reset(key)\n",
    "        return timestep.observation, timestep\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        timestep = self._env.step(state, action)\n",
    "        return timestep.observation, timestep, timestep.reward, timestep.is_done(), {}\n",
    "\n",
    "    def observation_space(self, params):\n",
    "        return spaces.Box(\n",
    "            low=self._env.observation_space.minimum,\n",
    "            high=self._env.observation_space.maximum,\n",
    "            shape=(np.prod(self._env.observation_space.shape),),\n",
    "            dtype=self._env.observation_space.dtype,\n",
    "        )\n",
    "\n",
    "    def action_space(self, params):\n",
    "        return spaces.Discrete(\n",
    "            num_categories=self._env.action_space.maximum.item() + 1,\n",
    "        )\n",
    "\n",
    "\n",
    "class ClipAction(GymnaxWrapper):\n",
    "    def __init__(self, env, low=-1.0, high=1.0):\n",
    "        super().__init__(env)\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        \"\"\"TODO: In theory the below line should be the way to do this.\"\"\"\n",
    "        # action = jnp.clip(action, self.env.action_space.low, self.env.action_space.high)\n",
    "        action = jnp.clip(action, self.low, self.high)\n",
    "        return self._env.step(key, state, action, params)\n",
    "\n",
    "\n",
    "class TransformObservation(GymnaxWrapper):\n",
    "    def __init__(self, env, transform_obs):\n",
    "        super().__init__(env)\n",
    "        self.transform_obs = transform_obs\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        return self.transform_obs(obs), state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
    "        return self.transform_obs(obs), state, reward, done, info\n",
    "\n",
    "\n",
    "class TransformReward(GymnaxWrapper):\n",
    "    def __init__(self, env, transform_reward):\n",
    "        super().__init__(env)\n",
    "        self.transform_reward = transform_reward\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, state, reward, done, info = self._env.step(key, state, action, params)\n",
    "        return obs, state, self.transform_reward(reward), done, info\n",
    "\n",
    "\n",
    "class VecEnv(GymnaxWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.reset = jax.vmap(self._env.reset, in_axes=(0, None))\n",
    "        self.step = jax.vmap(self._env.step, in_axes=(0, 0, 0, None))\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizeVecObsEnvState:\n",
    "    mean: jnp.ndarray\n",
    "    var: jnp.ndarray\n",
    "    count: float\n",
    "    env_state: environment.EnvState\n",
    "\n",
    "\n",
    "class NormalizeVecObservation(GymnaxWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=jnp.zeros_like(obs),\n",
    "            var=jnp.ones_like(obs),\n",
    "            count=1e-4,\n",
    "            env_state=state,\n",
    "        )\n",
    "        batch_mean = jnp.mean(obs, axis=0)\n",
    "        batch_var = jnp.var(obs, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            env_state=state.env_state,\n",
    "        )\n",
    "\n",
    "        return (obs - state.mean) / jnp.sqrt(state.var + 1e-8), state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "\n",
    "        batch_mean = jnp.mean(obs, axis=0)\n",
    "        batch_var = jnp.var(obs, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecObsEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        return (\n",
    "            (obs - state.mean) / jnp.sqrt(state.var + 1e-8),\n",
    "            state,\n",
    "            reward,\n",
    "            done,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizeVecRewEnvState:\n",
    "    mean: jnp.ndarray\n",
    "    var: jnp.ndarray\n",
    "    count: float\n",
    "    return_val: float\n",
    "    env_state: environment.EnvState\n",
    "\n",
    "\n",
    "class NormalizeVecReward(GymnaxWrapper):\n",
    "    def __init__(self, env, gamma):\n",
    "        super().__init__(env)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, state = self._env.reset(key, params)\n",
    "        batch_count = obs.shape[0]\n",
    "        state = NormalizeVecRewEnvState(\n",
    "            mean=0.0,\n",
    "            var=1.0,\n",
    "            count=1e-4,\n",
    "            return_val=jnp.zeros((batch_count,)),\n",
    "            env_state=state,\n",
    "        )\n",
    "        return obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "        return_val = state.return_val * self.gamma * (1 - done) + reward\n",
    "\n",
    "        batch_mean = jnp.mean(return_val, axis=0)\n",
    "        batch_var = jnp.var(return_val, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "\n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "\n",
    "        state = NormalizeVecRewEnvState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            return_val=return_val,\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        return obs, state, reward / jnp.sqrt(state.var + 1e-8), done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "# from wrappers import (\n",
    "#     VecEnv,\n",
    "#     NormalizeVecObservation,\n",
    "#     NormalizeVecReward,\n",
    "#     ClipAction,\n",
    "# )\n",
    "from chewc.gym import StoaEnv\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_logtstd = self.param(\"log_std\", nn.initializers.zeros, (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(actor_mean, jnp.exp(actor_logtstd))\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "\n",
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    \n",
    "    # Use your custom environment\n",
    "    env = StoaEnv()\n",
    "    env_params = {} # Your environment doesn't have params\n",
    "\n",
    "    # Add wrappers\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "    env = ClipAction(env)\n",
    "    env = VecEnv(env)\n",
    "    if config[\"NORMALIZE_ENV\"]:\n",
    "        env = NormalizeVecObservation(env)\n",
    "        env = NormalizeVecReward(env, config[\"GAMMA\"])\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "        # INIT NETWORK\n",
    "        network = ActorCritic(\n",
    "            env.action_space(env_params).shape[0], activation=config[\"ACTIVATION\"]\n",
    "        )\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi, value = network.apply(train_state.params, last_obs)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = env.step(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_state, env_state, last_obs, rng = runner_state\n",
    "            _, last_val = network.apply(train_state.params, last_obs)\n",
    "\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of envs\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "            if config.get(\"DEBUG\"):\n",
    "\n",
    "                def callback(info):\n",
    "                    return_values = info[\"returned_episode_returns\"][\n",
    "                        info[\"returned_episode\"]\n",
    "                    ]\n",
    "                    timesteps = (\n",
    "                        info[\"timestep\"][info[\"returned_episode\"]] * config[\"NUM_ENVS\"]\n",
    "                    )\n",
    "                    for t in range(len(timesteps)):\n",
    "                        print(\n",
    "                            f\"global step={timesteps[t]}, episodic return={return_values[t]}\"\n",
    "                        )\n",
    "\n",
    "                jax.debug.callback(callback, metric)\n",
    "\n",
    "            runner_state = (train_state, env_state, last_obs, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"LR\": 3e-4,\n",
    "        \"NUM_ENVS\": 64,              # Drastically reduced for a minimal memory footprint\n",
    "        \"NUM_STEPS\": 10,             # Number of steps per environment per update\n",
    "        \"TOTAL_TIMESTEPS\": 5e3,      # Very short run for just a few updates\n",
    "        \"UPDATE_EPOCHS\": 4,\n",
    "        \"NUM_MINIBATCHES\": 8,        # Adjusted to be a divisor of the batch size (64 * 10 = 640)\n",
    "        \"GAMMA\": 0.99,\n",
    "        \"GAE_LAMBDA\": 0.95,\n",
    "        \"CLIP_EPS\": 0.2,\n",
    "        \"ENT_COEF\": 0.0,\n",
    "        \"VF_COEF\": 0.5,\n",
    "        \"MAX_GRAD_NORM\": 0.5,\n",
    "        \"ACTIVATION\": \"tanh\",\n",
    "        \"ANNEAL_LR\": True,\n",
    "        \"NORMALIZE_ENV\": True,\n",
    "        \"DEBUG\": True,\n",
    "    }\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    train_jit = jax.jit(make_train(config))\n",
    "    out = train_jit(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98282b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
