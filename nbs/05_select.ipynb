{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cac6389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44eccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glect/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "\n",
    "import jax\n",
    "from jax import lax\n",
    "import jax.numpy as jnp\n",
    "from typing import Tuple\n",
    "from chewc.meiosis import create_gamete\n",
    "from chewc.structs import GeneticMap, Population\n",
    "from jax.tree_util import tree_map\n",
    "\n",
    "from functools import partial\n",
    "@partial(jax.jit, static_argnames=(\"k\",))\n",
    "def select_top_k(population: Population, values: jnp.ndarray, k: int) -> Population:\n",
    "    \"\"\"Select top-k individuals by `values` (1-D).\"\"\"\n",
    "    vals = jnp.ravel(values).astype(jnp.float32)  # (n_ind,)\n",
    "    _, top_idx = lax.top_k(vals, k)               # (k,)\n",
    "    return tree_map(lambda x: x[top_idx], population)\n",
    "\n",
    "\n",
    "\n",
    "def _solve_for_c_internal(lambda_val: float, g: jnp.ndarray, A: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Internal MME solver for OCS.\"\"\"\n",
    "    N = g.shape[0]\n",
    "    top_left = 2 * lambda_val * A\n",
    "    top_right = jnp.ones((N, 1))\n",
    "    bottom_left = jnp.ones((1, N))\n",
    "    bottom_right = jnp.zeros((1, 1))\n",
    "    M_top = jnp.hstack([top_left, top_right])\n",
    "    M_bottom = jnp.hstack([bottom_left, bottom_right])\n",
    "    M = jnp.vstack([M_top, M_bottom])\n",
    "    b = jnp.concatenate([g, jnp.array([1.0])])\n",
    "    solution = jnp.linalg.solve(M, b)\n",
    "    return solution[:-1]\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=('max_iterations',))\n",
    "def find_optimal_contributions(\n",
    "    g: jnp.ndarray,\n",
    "    A: jnp.ndarray,\n",
    "    delta_F: float,\n",
    "    start_lambda: float = 1.0,\n",
    "    tolerance: float = 1e-6,\n",
    "    max_iterations: int = 100\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    JIT-compiled function using jax.lax.while_loop to find optimal contributions.\n",
    "    \"\"\"\n",
    "    initial_state = (0, start_lambda, jnp.inf)\n",
    "\n",
    "    def condition_fn(state):\n",
    "        i, _, error = state\n",
    "        return jnp.logical_and(i < max_iterations, error > tolerance)\n",
    "\n",
    "    def body_fn(state):\n",
    "        i, lambda_val, _ = state\n",
    "        c = _solve_for_c_internal(lambda_val, g, A)\n",
    "        delta_F_current = 0.5 * c.T @ A @ c\n",
    "        error = jnp.abs(delta_F_current - delta_F)\n",
    "        new_lambda = lambda_val * (delta_F_current / delta_F)\n",
    "        return (i + 1, new_lambda, error)\n",
    "\n",
    "    _, final_lambda, _ = lax.while_loop(condition_fn, body_fn, initial_state)\n",
    "\n",
    "    final_c = _solve_for_c_internal(final_lambda, g, A)\n",
    "    return final_c\n",
    "\n",
    "\n",
    "def select_with_ocs(\n",
    "    population: Population,\n",
    "    breeding_values: jnp.ndarray,\n",
    "    delta_F: float,\n",
    "    k: int\n",
    ") -> Tuple[Population, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Selects top-k individuals and calculates their optimal contributions.\n",
    "    \"\"\"\n",
    "    # 1. Pre-select top-k candidates to reduce matrix sizes\n",
    "    candidates = select_top_k(population, breeding_values, k=k)\n",
    "    candidate_bvs = select_top_k(breeding_values, breeding_values, k=k)\n",
    "\n",
    "    # 2. Build the full A matrix for the candidates\n",
    "    # NOTE: This is a simplified approach. For larger `k`, a more\n",
    "    # efficient method to build A would be needed.\n",
    "    remapped_pedigree = remap_pedigree(candidates.meta)\n",
    "    A_inv_sparse = build_a_inverse_sparse(remapped_pedigree)\n",
    "    A = jnp.linalg.inv(A_inv_sparse.todense())\n",
    "\n",
    "    # 3. Find optimal contributions\n",
    "    c = find_optimal_contributions(candidate_bvs, A, delta_F)\n",
    "    \n",
    "    # 4. Normalize contributions to be non-negative and sum to 1\n",
    "    contributions = jnp.maximum(0, c)\n",
    "    contributions /= jnp.sum(contributions)\n",
    "\n",
    "    return candidates, contributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c9b42",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af484363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
